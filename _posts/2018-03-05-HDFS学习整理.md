---
layout:     post
title:      HDFS学习整理
date:       2018-3-5
author:     yunzhs
header-img: img/Fate of Princess.jpg
catalog: true
tags:
    - hadoop
typora-root-url: ..
typora-copy-images-to: ..\img\posts
---

# 一.HDFS入门

## 1.HDFS基本概念 

### 1.1.HDFS 介绍

​	HDFS 是 Hadoop Distribute File System 的简称，意为：Hadoop 分布式文件系统。是 Hadoop 核心组件之

一，作为最底层的分布式存储服务而存在。

​	分布式文件系统解决的问题就是大数据存储。它们是横跨在多台计算机上的存储系统。分布式文件系统在大

数据时代有着广泛的应用前景，它们为存储和处理超大规模数据提供所需的扩展能力。

### 1.2.HDFS设计目标

1) 硬件故障是常态， HDFS 将有成百上千的服务器组成，每一个组成部分都有可能出现故障。因此故障的检测和自动快速恢复是 HDFS 的核心架构目标。

2) HDFS 上的应用与一般的应用不同，它们主要是以流式读取数据。HDFS 被设计成适合批量处理，而不是用户交互式的。相较于数据访问的反应时间，更注重数据访问的高吞吐量。

3) 典型的 HDFS 文件大小是 GB 到 TB 的级别。所以，HDFS 被调整成支持大文件。它应该提供很高的聚合数据带宽，一个集群中支持数百个节点，一个集群中还应该支持千万级别的文件。

4) 大部分 HDFS 应用对文件要求的是 write-one-read-many 访问模型。一个文件一旦创建、写入、关闭之后就不需要修改了。这一假设简化了数据一致性问题，使高吞吐量的数据访问成为可能。

5) 移动计算的代价比之移动数据的代价低。一个应用请求的计算，离它操作的数据越近就越高效，这在数据达到海量级别的时候更是如此。将计算移动到数据附近，比之将数据移动到应用所在显然更好。

6) 在异构的硬件和软件平台上的可移植性。这将推动需要大数据集的应用更广泛地采用 HDFS 作为平台。

## 2.HDFS重要特性

​	首先，它是一个文件系统，用于存储文件，通过统一的命名空间目录树来定位文件；

​	其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。

### 2.1.master/slave 架构

​	HDFS 采用 master/slave 架构。一般一个 HDFS 集群是有一个 Namenode 和一定数目的 Datanode 组成。Namenode 是 HDFS 集群主节点，Datanode 是 HDFS 集群从节点，两种角色各司其职，共同协调完成分布式的文件存储服务。

### 2.2. 分块存储

​	HDFS 中的文件在物理上是分块存储（block）的，块的大小可以通过配置参数来规定，默认大小在 hadoop2.x 版本中是 128M。

### 2.3.名字空间（NameSpace ）

​	HDFS 支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。
​	Namenode 负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被 Namenode 记录下来。
​	HDFS 会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，
形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。

### 2.4.Namenode 元数据管理

​	我们把目录结构及文件分块位置信息叫做元数据。Namenode 负责维护整个hdfs 文件系统的目录树结构，以

及每一个文件所对应的 block 块信息（block 的id，及所在的 datanode 服务器）。

### 2.5.Datanode 数据存储

​	文件的各个 block 的具体存储管理由 datanode 节点承担。每一个 block 都可以在多个 datanode 上。Datanode 需要定时向 Namenode 汇报自己持有的 block信息。
​	存储多个副本（副本数量也可以通过参数设置 dfs.replication，默认是 3）。

### 2.6.副本机制

​	为了容错，文件的所有 block 都会有副本。每个文件的 block 大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。

### 2.7. 一次写入，多次读出

​	HDFS 是设计成适应一次写入，多次读出的场景，且不支持文件的修改。正因为如此，HDFS 适合用来做大数据分析的底层存储服务，并不适合用来做.网盘等应用，因为，修改不方便，延迟大，网络开销大，成本太高。



## 3.HDFS基本操作

### 3.1.Shell 命令行客户端

Hadoop 提供了文件系统的 shell 命令行客户端，使用方法如下：

````
hadoop fs <args>
````

hadoop fs <args>文件系统 shell 包括与 Hadoop 分布式文件系统（HDFS）以及 Hadoop 支持
的其他文件系统（如本地 FS，HFTP FS，S3 FS 等）直接交互的各种类似 shell
的命令。所有 FS shell 命令都将路径 URI 作为参数。
URI 格式为 scheme://authority/path。对于 HDFS，该 scheme 是 hdfs，对于本地 FS，该 scheme 是 file。scheme 和 authority 是可选的。如果未指定，则使用配置中指定的默认方案。
对于 HDFS,命令示例如下：
hadoop fs -ls hdfs://namenode:host/parent/child
hadoop fs -ls /parent/child fs.defaultFS 中有配置
对于本地文件系统，命令示例如下：
hadoop fs -ls file:///root/
如果使用的文件系统是 HDFS，则使用 hdfs dfs 也是可以的，此时
hadoop fs <args> = hdfs dfs <args>

### 3.2.Shell 命令选项

![1520248881736](/img/posts/1520248881736.png)

### 3.3.Shell 常用命令介绍

- ls
  使用方法：hadoop fs -ls [-h] [-R] <args>
  功能：显示文件、目录信息。
  示例：hadoop fs -ls /user/hadoop/file1
- mkdir
  使用方法：hadoop fs -mkdir [-p] <paths>
  功能：在 hdfs 上创建目录，-p 表示会创建路径中的各级父目录。
  示例：hadoop fs -mkdir –p /user/hadoop/dir1
- put
  使用方法：hadoop fs -put [-f] [-p] [ -|<localsrc1> .. ]. <dst>
  功能：将单个 src 或多个 srcs 从本地文件系统复制到目标文件系统。
  -p：保留访问和修改时间，所有权和权限。
  -f：覆盖目的地（如果已经存在）
  示例：hadoop fs -put -f localfile1 localfile2 /user/hadoop/hadoopdir
- get
  使用方法：hadoop fs -get [-ignorecrc] [-crc] [-p] [-f] <src> <localdst>
  -ignorecrc：跳过对下载文件的 CRC 检查。
  -crc：为下载的文件写 CRC 校验和。
  功能：将文件复制到本地文件系统。
  示例：hadoop fs -get hdfs://host:port/user/hadoop/file localfile
- appendToFile
  使用方法：hadoop fs -appendToFile <localsrc> ... <dst>
  功能：追加一个文件到已经存在的文件末尾
  示例：hadoop fs -appendToFile localfile /hadoop/hadoopfile
- cat
  使用方法：hadoop fs -cat [-ignoreCrc] URI [URI ...]
  功能：显示文件内容到 stdout
  示例：hadoop fs -cat /hadoop/hadoopfile
- tail
  使用方法：hadoop fs -tail [-f] URI
  功能：将文件的最后一千字节内容显示到 stdout。
  -f 选项将在文件增长时输出附加数据。
  示例：hadoop fs -tail /hadoop/hadoopfile
- chgrp
  使用方法：hadoop fs -chgrp [-R] GROUP URI [URI ...]
  功能：更改文件组的关联。用户必须是文件的所有者，否则是超级用户。
  -R 将使改变在目录结构下递归进行。
  示例：hadoop fs -chgrp othergroup /hadoop/hadoopfile
- -c c hmod
  功能：改变文件的权限。使用-R 将使改变在目录结构下递归进行。
  示例：hadoop fs -chmod 666 /hadoop/hadoopfile
- -c c hown
  功能：改变文件的拥有者。使用-R 将使改变在目录结构下递归进行。
  示例：hadoop fs -chown someuser:somegrp /hadoop/hadoopfile
- copyFromLocal
  使用方法：hadoop fs -copyFromLocal <localsrc> URI
  功能：从本地文件系统中拷贝文件到 hdfs 路径去
  示例：hadoop fs -copyFromLocal /root/1.txt /
- copyToLocal
  功能：从 hdfs 拷贝到本地
  示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz


- cp
  功能：从 hdfs 的一个路径拷贝 hdfs 的另一个路径
  示例： hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2
- mv
  功能：在 hdfs 目录中移动文件
  示例： hadoop fs -mv /aaa/jdk.tar.gz /
- getmerge
  功能：合并下载多个文件
  示例：比如 hdfs 的目录 /aaa/下有多个文件:log.1, log.2,log.3,...
  hadoop fs -getmerge /aaa/log.* ./log.sum
- rm
  功能：删除指定的文件。只删除非空目录和文件。-r 递归删除。
  示例：hadoop fs -rm -r /aaa/bbb/
- df
  功能：统计文件系统的可用空间信息
  示例：hadoop fs -df -h /
- du
  功能：显示目录中所有文件大小，当只指定一个文件时，显示此文件的大小。
  示例：hadoop fs -du /user/hadoop/dir1
- setrep
  功能：改变一个文件的副本系数。-R 选项用于递归改变目录下所有文件的副本
  系数。
  示例：hadoop fs -setrep -w 3 -R /user/hadoop/dir1

# 二.HDFS基本原理

## 1.NameNode概述

a、 NameNode 是 HDFS 的核心。

b、 NameNode 也称为 Master。

c、 NameNode 仅存储 HDFS 的元数据：文件系统中所有文件的目录树，并跟踪整个集群中的文件。

d、 NameNode 不存储实际数据或数据集。数据本身实际存储在 DataNodes 中。

e、 NameNode 知道 HDFS 中任何给定文件的块列表及其位置。使用此信息NameNode 知道如何从块中构建文    件

f、 NameNode 并不持久化存储每个文件中各个块所在的 DataNode 的位置信息，这些信息会在系统启动时从数据节点重建。

g、 NameNode 对于 HDFS 至关重要，当 NameNode 关闭时，HDFS / Hadoop 集群无法访问。

h、 NameNode 是 Hadoop 集群中的单点故障。

i、 NameNode 所在机器通常会配置有大量内存（RAM）。



## 2.DataNode 概述

a、 DataNode 负责将实际数据存储在 HDFS 中。

b、 DataNode 也称为 Slave。

c、 NameNode 和 DataNode 会保持不断通信。

d、 DataNode 启动时，它将自己发布到 NameNode 并汇报自己负责持有的块列表。

e、 当某个 DataNode 关闭时，它不会影响数据或群集的可用性。NameNode 将安排由其他 DataNode 管理的块进行副本复制。

f、 DataNode 所在机器通常配置有大量的硬盘空间。因为实际数据存储在DataNode 中。

g、 DataNode 会定期（dfs.heartbeat.interval 配置项配置，默认是 3 秒）向NameNode 发送心跳，如果 NameNode 长时间没有接受到 DataNode 发送的心跳， NameNode 就会认为该 DataNode 失效。

h、 block 汇报时间间隔取参数 dfs.blockreport.intervalMsec,参数未配置的话默认为 6 小时.

## 3.HDFS的工作机制

​	NameNode 负责管理整个文件系统元数据；DataNode 负责管理具体文件数据块存储；Secondary 

NameNode 协助 NameNode 进行元数据的备份。

​	HDFS 的内部工作机制对客户端保持透明，客户端请求访问 HDFS 都是通过向NameNode 申请来进行。

### 3.1 HDFS写数据流程

![hdfs写数据流程](/img/posts/hdfs写数据流程-0336625040.png)

详细步骤解析：

1、 client 发起文件上传请求，通过 RPC 与 NameNode 建立通讯，NameNode检查目标文件是否已存在，父目录是否存在，返回是否可以上传；

2、 client 请求第一个 block 该传输到哪些 DataNode 服务器上；

3、 NameNode 根据配置文件中指定的备份数量及机架感知原理进行文件分配，返回可用的 DataNode 的地址如：A，B，C；
注：Hadoop 在设计时考虑到数据的安全与高效，数据文件默认在 HDFS 上存放三份，存储策略为本地一份，同机架内其它某一节点上一份，不同机架的某一节点上一份。

4、 client 请求 3 台 DataNode 中的一台 A 上传数据（本质上是一个 RPC 调用，建立 pipeline），A 收到请求会继续调用 B，然后 B 调用 C，将整个pipeline 建立完成，后逐级返回 client；

5、 client 开始往 A 上传第一个 block（先从磁盘读取数据放到一个本地内存缓存），以 packet 为单位（默认 64K），A 收到一个 packet 就会传给 B，B 传给 C；A 每传一个 packet 会放入一个应答队列等待应答。

6、 数据被分割成一个个 packet 数据包在 pipeline 上依次传输，在pipeline 反方向上，逐个发送 ack（命令正确应答），最终由 pipeline中第一个 DataNode 节点 A 将 pipeline ack 发送给 client;

7、 当一个 block 传输完成之后，client 再次请求 NameNode 上传第二个block 到服务器。hdfs写数据流程



### 3.2 HDFS 读数据流程

![hdfs读数据流程](/img/posts/hdfs读数据流程.png)

详细步骤解析：
1、 Client 向 NameNode 发起 RPC 请求，来确定请求文件 block 所在的位置；

2、 NameNode会视情况返回文件的部分或者全部block列表，对于每个block，NameNode 都会返回含有该 block 副本的 DataNode 地址；

3、 这些返回的 DN 地址，会按照集群拓扑结构得出 DataNode 与客户端的距离，然后进行排序，排序两个规则：网络拓扑结构中距离 Client 近的排靠前；心跳机制中超时汇报的 DN 状态为 STALE，这样的排靠后；

4、 Client 选取排序靠前的 DataNode 来读取 block，如果客户端本身就是DataNode,那么将从本地直接获取数据；

5、 底层上本质是建立 Socket Stream（FSDataInputStream），重复的调用父类 DataInputStream 的 read 方法，直到这个块上的数据读取完毕；

6、 当读完列表的 block 后，若文件读取还没有结束，客户端会继续向NameNode 获取下一批的 block 列表；

7、 读取完一个 block 都会进行 checksum 验证，如果读取 DataNode 时出现错误，客户端会通知 NameNode，然后再从下一个拥有该 block 副本的DataNode 继续读。

8、 read 方法是并行的读取 block 信息，不是一块一块的读取；NameNode 只是返回Client请求包含块的DataNode地址，并不是返回请求块的数据；

9、 最终读取来所有的 block 会合并成一个完整的最终文件。



# 三.HDFS的应用开发

## 1.HDFS的JAVA API操作

 	HDFS 在生产应用中主要是客户端的开发，其核心步骤是从 HDFS 提供的 api中构造一个 HDFS 的访问客户端对象，然后通过该客户端对象操作（增删改查）HDFS 上的文件。

### 1.1需求分析

​	将本地的文件上传到服务器上的hdfs指定的目录中

### 1.2 搭建开发环境

创建 Maven 工程，引入 pom 依赖

```
<dependencies>
     <dependency>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-common</artifactId>
     <version>2.7.4</version>
     </dependency>
     <dependency>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-hdfs</artifactId>
     <version>2.7.4</version>
     </dependency>
     <dependency>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-client</artifactId>
     <version>2.7.4</version>
     </dependency>
</dependencies>
```

#### 配置 windows 平台 Hadoop 环境

​	在 windows 上做 HDFS 客户端应用开发，需要设置 Hadoop 环境,而且要求是windows 平台编译的 Hadoop,不然会报以下的错误:
Failed to locate the winutils binary in the hadoop binary path java.io.IOException: Could not
locate executable null\bin\winutils.exe in the Hadoop binaries.

为此我们需要进行如下的操作：
A、在 windows 平台下编译 Hadoop 源码（可以参考资料编译，但不推荐）

B、使用已经编译好的 Windows 版本 Hadoop：hadoop-2.7.4-with-windows.tar.gz

C、解压一份到 windows 的任意一个目录下

D、在 windows 系统中配置 HADOOP_HOME 指向你解压的安装包目录

E、在 windows 系统的 path 变量中加入 HADOOP_HOME 的 bin 目录

在cmd命令行中输入hadoop,显示命令即表示命名成功

注意这个hadoop要求hadoop和java的所在目录都不能在有中文和空格的目录中

最骚的是,即使我都解决了但还是出现了这样的错误,最后只能在代码中嵌入

```
        System.setProperty("hadoop.home.dir", "C:\\Windows\\hadoop-2.7.4");
```

### 1.3最终代码

```
public class HDFS_TEST {
    public static void main(String[] args) throws IOException {
        Configuration conf = new Configuration();
//这里指定使用的是 hdfs 文件系统
        System.setProperty("hadoop.home.dir", "C:\\Windows\\hadoop-2.7.4");
        conf.set("fs.defaultFS", "hdfs://192.168.217.228:9000");
//通过如下的方式进行客户端身份的设置
        System.setProperty("HADOOP_USER_NAME", "root");
//通过 FileSystem 的静态方法获取文件系统客户端对象
        FileSystem fs = FileSystem.get(conf);
//也可以通过如下的方式去指定文件系统的类型 并且同时设置用户身份
//FileSystem fs = %JAVA_HOME%FileSystem.get(new URI("hdfs://192.168.217.228:9000"), conf, "root");
//创建一个目录
        fs.create(new Path("/miao1/"), false);
//上传一个文件
        //fs.copyFromLocalFile(new Path("E:\\yunzhs.txt"), new Path("/cn/yunzhs/"));
//关闭我们的文件系统
        fs.close();
    }
}
```

注意:

创建一个目录,如/cn/yunzhs/miao/,其中cn和yunzhs是目录,而miao则不是目录,只是一个不可执行的文件

,可见这儿的create并不是一个目录,实际含义并未深究

## 2.shell 定时采集至数据至 HDFS

### 2.1需求分析

​	上线的网站每天都会产生日志数据。假如有这样的需求：要求在凌晨 24 点开始操作前一天产生的日志文件，准实时上传至 HDFS 集群上。
​	该如何实现？实现后能否实现周期性上传需求？如何定时？

### 2.2关键代码

HDFS SHELL:

```
hadoop fs –put // 向hdfs上传文件
```

Linux crontab: :

```
crontab -e
0 0 * * * /shell/ uploadFile2Hdfs.sh //每天凌晨 12：00 执行一次
```

### 2.3流程分析

​	一般日志文件生成的逻辑由业务系统决定，比如每小时滚动一次，或者一定大小滚动一次，避免单个日志文件过大不方便操作。
​	比如滚动后的文件命名为 access.log.x,其中 x 为数字。正在进行写的日志文件叫做 access.log。这样的话，如果日志文件后缀是 1\2\3 等数字，则该文件满足需求可以上传，就把该文件移动到准备上传的工作区间目录。工作区有文件之后，可以使用 hadoop put 命令将文件上传。

### 2.4具体代码

uploadFile2Hdfs.sh:

```
#!/bin/bash

#set java env
export JAVA_HOME=/export/servers/jdk/bin/java
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH

#set hadoop env
export HADOOP_HOME=/export/servers/hadoop/bin/hadoop
export PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH


#日志文件存放的目录
log_src_dir=/export/data/test

#待上传文件存放的目录
log_toupload_dir=/export/data/test-result


#日志文件上传到hdfs的根路径
date1=`date -d last-day +%Y_%m_%d`
hdfs_root_dir=/data/clickLog/$date1/ 

#打印环境变量信息
echo "envs: hadoop_home: $HADOOP_HOME"


#读取日志文件的目录，判断是否有需要上传的文件
echo "log_src_dir:"$log_src_dir
ls $log_src_dir | while read fileName
do
	if [[ "$fileName" == access.log.* ]]; then
	# if [ "access.log" = "$fileName" ];then
		date=`date +%Y_%m_%d_%H_%M_%S`
		#将文件移动到待上传目录并重命名
		#打印信息
		echo "moving $log_src_dir$fileName to $log_toupload_dir"xxxxx_click_log_$fileName"$date"
		mv $log_src_dir$fileName $log_toupload_dir"xxxxx_click_log_$fileName"$date
		#将待上传的文件path写入一个列表文件willDoing
		echo $log_toupload_dir"xxxxx_click_log_$fileName"$date >> $log_toupload_dir"willDoing."$date
	fi
	
done
#找到列表文件willDoing
ls $log_toupload_dir | grep will |grep -v "_COPY_" | grep -v "_DONE_" | while read line
do
	#打印信息
	echo "toupload is in file:"$line
	#将待上传文件列表willDoing改名为willDoing_COPY_
	mv $log_toupload_dir$line $log_toupload_dir$line"_COPY_"
	#读列表文件willDoing_COPY_的内容（一个一个的待上传文件名）  ,此处的line 就是列表中的一个待上传文件的path
	cat $log_toupload_dir$line"_COPY_" |while read line
	do
		#打印信息
		echo "puting...$line to hdfs path.....$hdfs_root_dir"
		hadoop fs -mkdir -p $hdfs_root_dir
		hadoop fs -put $line $hdfs_root_dir
	done	
	mv $log_toupload_dir$line"_COPY_"  $log_toupload_dir$line"_DONE_"
done

```

# 附录

hdfs操作API

```
package cn.itcast.bigdata.hdfs;

import java.io.FileNotFoundException;
import java.io.IOException;
import java.net.URI;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.BlockLocation;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.LocatedFileStatus;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.RemoteIterator;
import org.junit.Before;
import org.junit.Test;

public class HdfsClient {

	FileSystem fs = null;

	@Before
	public void init() throws Exception {

		// 构造一个配置参数对象，设置一个参数：我们要访问的hdfs的URI
		// 从而FileSystem.get()方法就知道应该是去构造一个访问hdfs文件系统的客户端，以及hdfs的访问地址
		// new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml
		// 然后再加载classpath下的hdfs-site.xml
		Configuration conf = new Configuration();
		// conf.set("fs.defaultFS", "hdfs://mini1:9000");
		/**
		 * 参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置
		 */
		/*conf.set("dfs.replication", "2");
		conf.set("dfs.block.size", "64m");*/

		// 获取一个hdfs的访问客户端，根据参数，这个实例应该是DistributedFileSystem的实例
		// fs = FileSystem.get(conf);

		// 如果这样去获取，那conf里面就可以不要配"fs.defaultFS"参数，而且，这个客户端的身份标识已经是root用户
		fs = FileSystem.get(new URI("hdfs://mini1:9000"), conf, "root");

		// 获取文件系统相关信息
		/*
		 * DatanodeInfo[] dataNodeStats = ((DistributedFileSystem)
		 * fs).getDataNodeStats(); for(DatanodeInfo dinfo: dataNodeStats){
		 * System.out.println(dinfo.getHostName()); }
		 */

	}

	/**
	 * 往hdfs上传文件
	 * 
	 * @throws Exception
	 */
	@Test
	public void testAddFileToHdfs() throws Exception {

		// 要上传的文件所在的本地路径

		// 要上传到hdfs的目标路径*/
		Path src = new Path("e:/mysql-connector-java-5.1.28.jar");
		Path dst = new Path("/");
		fs.copyFromLocalFile(src, dst);

		fs.close();
	}

	/**
	 * 从hdfs中复制文件到本地文件系统
	 * 
	 * @throws IOException
	 * @throws IllegalArgumentException
	 */
	@Test
	public void testDownloadFileToLocal() throws IllegalArgumentException, IOException {

		// fs.copyToLocalFile(new Path("/mysql-connector-java-5.1.28.jar"), new
		// Path("d:/"));
		fs.copyToLocalFile(false, new Path("/mysql-connector-java-5.1.28.jar"), new Path("d:/"), true);
		fs.close();

	}

	/**
	 * 目录操作
	 * 
	 * @throws IllegalArgumentException
	 * @throws IOException
	 */
	@Test
	public void testMkdirAndDeleteAndRename() throws IllegalArgumentException, IOException {

		// 创建目录
		fs.mkdirs(new Path("/a1/b1/c1"));

		// 删除文件夹 ，如果是非空文件夹，参数2必须给值true
		fs.delete(new Path("/aaa"), true);

		// 重命名文件或文件夹
		fs.rename(new Path("/a1"), new Path("/a2"));

	}

	/**
	 * 查看目录信息，只显示文件
	 * 
	 * @throws IOException
	 * @throws IllegalArgumentException
	 * @throws FileNotFoundException
	 */
	@Test
	public void testListFiles() throws FileNotFoundException, IllegalArgumentException, IOException {

		
		RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("/"), true);

		while (listFiles.hasNext()) {

			LocatedFileStatus fileStatus = listFiles.next();

			System.out.println(fileStatus.getPath().getName());
			System.out.println(fileStatus.getBlockSize());
			System.out.println(fileStatus.getPermission());
			System.out.println(fileStatus.getLen());
			BlockLocation[] blockLocations = fileStatus.getBlockLocations();
			for (BlockLocation bl : blockLocations) {
				System.out.println("block-length:" + bl.getLength() + "--" + "block-offset:" + bl.getOffset());
				String[] hosts = bl.getHosts();
				for (String host : hosts) {
					System.out.println(host);
				}

			}

			System.out.println("--------------打印的分割线--------------");

		}

	}

	/**
	 * 查看文件及文件夹信息
	 * 
	 * @throws IOException
	 * @throws IllegalArgumentException
	 * @throws FileNotFoundException
	 */
	@Test
	public void testListAll() throws FileNotFoundException, IllegalArgumentException, IOException {

		FileStatus[] listStatus = fs.listStatus(new Path("/"));

		String flag = "";
		for (FileStatus fstatus : listStatus) {

			if (fstatus.isFile()) {
				flag = "f-- ";
			} else {
				flag = "d-- ";
			}
			System.out.println(flag + fstatus.getPath().getName());
			System.out.println(fstatus.getPermission());

		}

	}

}

```

