---
layout:     post
title:      spark计算模型RDD
date:       2017-10-08
author:     yunzhs
header-img: img/Fate of Princess.jpg
catalog: true
tags:
    - spark
typora-root-url: ..
typora-copy-images-to: ..\img\posts
---

# 一、 弹性分布式数据集RDD

## 1.RDD概述

### 1.1.什么是RDD?

RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个**不可****变、可分区、里面的元素可并行计算**的集合。RDD具有数据流模型的特点：**自动容错、位置感知性调度和可伸缩****性**。RDD允许用户在执行多个查询时显式地将数据缓存在内存中，后续的查询能够重用这些数据，这极大地提升了查询速度。

Dataset：一个数据集合，用于存放数据的。

Distributed：RDD中的数据是分布式存储的，可用于分布式计算。

Resilient：RDD中的数据可以存储在内存中或者磁盘中。

### 1.2.RDD的属性

1. A list of partitions：一个分区（Partition）列表，数据集的基本组成单位。

   ​       对于RDD来说，每个分区都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分区个数，如果没有指定，那么就会采用默认值。（比如：读取HDFS上数据文件产生的RDD分区数跟 block的个数相等）

2. A function for computing eachsplit ：一个计算每个分区的函数。

   park中RDD的计算是以分区为单位的，每个RDD都会实现compute函数以达到这个目的。

3. A list of dependencies on other RDDs：一个RDD会依赖于其他多个RDD，RDD之间的依赖关系。

   RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分 分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。

4. Optionally, a Partitioner forkey-value RDDs (e.g. to say that the RDD is hash-partitioned)：一个 Partitioner，即RDD的分区函数（可选项）。

   ​       当前Spark中实现了两种类型的分区函数，一个是基于哈希的HashPartitioner，另外一  个是基于范围的RangePartitioner。**只有对于key-value的RDD，才会有Partitioner**，**非key-value的RDD的Parititioner的****值是None**。Partitioner函数决定了parent RDD Shuffle输出时的分区数量。

5. Optionally, a list of preferredlocations to compute each split on (e.g. block locations for an HDFS file)：

   一个列表，存储每个Partition的优先位置(可选项)。	

   ​       对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置（spark进行任务分配的时候尽可能选择那些存有数据的worker节点来进行任务计算）。

   ### 这张图非常重要:

   ![词计数流程剖析RDD五大属](/img/posts/单词计数流程剖析RDD五大属性.png)

   ​

## 2.创建RDD

1）由一个已经存在的Scala集合创建。 

`val rdd1 =sc.parallelize(Array(1,2,3,4,5,6,7,8))` 

2）由外部存储系统的文件创建。包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等。

`val rdd2 =sc.textFile("/words.txt")`

3）已有的RDD经过算子转换生成新的RDD

`valrdd3=rdd2.flatMap(_.split(" "))`

## 3.RDD编程API

### 3.1.RDD算子分类

Transformation（转换）：根据数据集创建一个新的数据集，计算后返回一个新RDD；例如：一个rdd进行map操作后生了**一个新的rdd**。

Action（动作）：对rdd结果计算后返回**一个数值value**给驱动程序；

例如：collect算子将数据集的所有元素收集完成返回给驱动程序。

### 3.2.Transformation

​	RDD中的所有转换都是**延迟加载**的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark**更加有效率地运行**。



#### 常用的Transformation：

| **转换**                                   | **含义**                                   |
| ---------------------------------------- | ---------------------------------------- |
| **map(func)**                            | 返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成      |
| **filter(func)**                         | 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成 |
| **flatMap(func)**                        | 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素） |
| **mapPartitions(func)**                  | 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] => Iterator[U] |
| **mapPartitionsWithIndex(func)**         | 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是  (Int,  Interator[T]) => Iterator[U] |
| **union(otherDataset)**                  | 对源RDD和参数RDD求并集后返回一个新的RDD                 |
| **intersection(otherDataset)**           | 对源RDD和参数RDD求交集后返回一个新的RDD                 |
| **distinct([numTasks]))**                | 对源RDD进行去重后返回一个新的RDD                      |
| **groupByKey([numTasks])**               | 在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD |
| **reduceByKey(func, [numTasks])**        | 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置 |
| **sortByKey([ascending], [numTasks])**   | 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD |
| **sortBy(func,[ascending], [numTasks])** | 与sortByKey类似，但是更灵活                       |
| **join(otherDataset, [numTasks])**       | 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD |
| **cogroup(otherDataset, [numTasks])**    | 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD |
| **coalesce(numPartitions)**              | 减少 RDD 的分区数到指定值。                         |
| **repartition(numPartitions)**           | 重新给 RDD 分区                               |
| **repartitionAndSortWithinPartitions(partitioner)** | 重新给 RDD 分区，并且每个分区内以记录的 key 排序            |

### 3.3.Action

| **动作**                         | **含义**                                   |
| ------------------------------ | ---------------------------------------- |
| **reduce(func)**               | reduce将RDD中元素前两个传给输入函数，产生一个新的return值，新产生的return值与RDD中下一个元素（第三个元素）组成两个元素，再被传给输入函数，直到最后只有一个值为止。 |
| **collect()**                  | 在驱动程序中，以数组的形式返回数据集的所有元素                  |
| **count()**                    | 返回RDD的元素个数                               |
| **first()**                    | 返回RDD的第一个元素（类似于take(1)）                  |
| **take(n)**                    | 返回一个由数据集的前n个元素组成的数组                      |
| **takeOrdered(n, [ordering])** | 返回自然顺序或者自定义顺序的前 n 个元素                    |
| **saveAsTextFile(path)**       | 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本 |
| **saveAsSequenceFile(path)**   | 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。 |
| **saveAsObjectFile(path)**     | 将数据集的元素，以 Java 序列化的方式保存到指定的目录下           |
| **countByKey()**               | 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 |
| **foreach(func)**              | 在数据集的每一个元素上，运行函数func                     |
| **foreachPartition(func)**     | 在数据集的每一个分区上，运行函数func                     |

## 4.RDD的依赖关系

### 4.1.RDD的依赖

RDD和它依赖的父RDD的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。

![52256891776](/img/posts/1522568917767.png)

### 4.2.窄依赖

窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用

总结：窄依赖我们形象的比喻为**独生子女**

### 4.3.宽依赖

宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition

总结：宽依赖我们形象的比喻为**超生** 

### 4.4.Lineage（血统）

会记录下当前RDD上的元数据信息,以及对应作用在RDD上的转换行为.如果当前某一个RDD某个分区数据丢失指挥,它是可以根据血统来进行恢复.



## 5.RDD的缓存

Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或者缓存数据集。当持久化某个RDD后，每一个节点都将把计算分区结果保存在内存中，对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。

### 5.1.RDD的缓存方式

​	RDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是**触发后面的action**时，该RDD将会被缓存在计算节点的内存中，并供后面重用。

![52258690018](/img/posts/1522586900188.png)

​	

​	通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在object StorageLevel中定义的。

![52258693725](/img/posts/1522586937254.png)

​	缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。

​	删除缓存数据

- rdd.unpersist(boolean)
  - ture 表示阻塞一直等待缓存数据被删除
  - false 表示不阻塞，边删除边进行下面的相关操作

## 6.DAG的生成

### 6.1.什么是DAG

​	DAG(Directed Acyclic Graph)叫做**有向无环图**，原始的RDD通过一系列的转换就形成了DAG，根据RDD之间依赖关系的不同将DAG划分成不同的Stage(调度阶段)。对于窄依赖，partition的转换处理在一个Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。

![52258777664](/img/posts/1522587776647.png)

stage遇到宽依赖就生成一个新的stage

## 7.Spark任务调度

### 7.1.任务调度流程图

![52259875241](/img/posts/1522598752414.png)

各个RDD之间存在着依赖关系，这些依赖关系就形成有向无环图DAG，DAGScheduler对这些依赖关系形成的DAG进行Stage划分，划分的规则很简单，从后往前回溯，遇到窄依赖加入本stage，遇见宽依赖进行Stage切分。完成了Stage的划分。DAGScheduler基于每个Stage生成TaskSet,并将TaskSet提交给TaskScheduler。TaskScheduler 负责具体的task调度,最后在Worker节点上启动task。

### 7.2.DAGScheduler

（1）DAGScheduler对DAG有向无环图进行Stage划分。

（2）记录哪个RDD或者 Stage 输出被物化（缓存），通常在一个复杂的shuffle之后，通常物化一下(cache、persist)，方便之后的计算。

（3）重新提交shuffle输出丢失的stage（stage内部计算出错）给TaskScheduler

（4）将 Taskset 传给底层调度器

​	a）– spark-cluster TaskScheduler

​	b）– yarn-cluster YarnClusterScheduler

​	c）– yarn-client YarnClientClusterScheduler

### 7.3.TaskScheduler

（1）为每一个TaskSet构建一个TaskSetManager 实例管理这个TaskSet 的生命周期

（2）数据本地性决定每个Task最佳位置

（3）提交 taskset( 一组task) 到集群运行并监控

（4）推测执行，碰到计算缓慢任务需要放到别的节点上重试

（5）重新提交Shuffle输出丢失的Stage给DAGScheduler

# 二.RDD容错机制之checkpoint

## 1.checkpoint是什么

   (1）、Spark 在生产环境下经常会面临transformation的RDD非常多（例如一个Job中包含1万个RDD）或者具体transformation的RDD本身计算特别复杂或者耗时（例如计算时长超过1个小时），这个时候就要考虑对计算结果数据持久化保存；

（2）、Spark是擅长多步骤迭代的，同时擅长基于Job的复用，这个时候如果能够对曾经计算的过程产生的数据进行复用，就可以极大的提升效率；

（3）、如果采用persist把数据放在内存中，虽然是快速的，但是也是最不可靠的；如果把数据放在磁盘上，也不是完全可靠的！**例如磁盘会损坏，系统管理员可能清空磁盘**。

（4）、Checkpoint的产生就是为了相对而言更加可靠的持久化数据，在Checkpoint的时候可以指定把数据放在本地，并且是多副本的方式，但是在生产环境下是放在HDFS上，这就天然的借助了HDFS高容错、高可靠的特征来完成了最大化的可靠的持久化数据的方式；

假如进行一个1万个算子操作，在9000个算子的时候persist，数据还是有可能丢失的，但是如果checkpoint，数据丢失的概率几乎为0。

### 2.checkpoint原理机制

（1）当RDD使用cache机制从内存中读取数据，如果数据没有读到，会使用checkpoint机制读取数据。此时如果没有checkpoint机制，那么就需要找到父RDD重新计算数据了，因此checkpoint是个很重要的容错机制。

​	 checkpoint就是对于一个RDD [chain]()（链）如果后面需要反复使用某些中间结果RDD，可能因为一些故障导致该中间数据丢失，那么就可以针对该RDD启动checkpoint机制，使用checkpoint首先需要调用sparkContext的setCheckpoint方法，设置一个容错文件系统目录，比如hdfs，然后对RDD调用checkpoint方法。之后在RDD所处的job运行结束后，会启动一个单独的job来将checkpoint过的数据写入之前设置的文件系统持久化，进行高可用。所以后面的计算在使用该RDD时，如果数据丢失了，但是还是可以从它的checkpoint中读取数据，不需要重新计算。

```
利用sc.setCheckpointDir(HDFS目录)
rdd.checkpoint //针对于需要持久的RDD调用checkpoint方法
后面也需要触发action的算子操作
```

（2）persist或者cache与checkpoint的区别在于,前者持久化只是将数据保存在BlockManager中但是其lineage是不变的，但是后者checkpoint执行完后，rdd已经没有依赖RDD，只有一个checkpointRDD，checkpoint之后，RDD的lineage就改变了。persist或者cache持久化的数据丢失的可能性更大，因为可能磁盘或内存被清理，但是checkpoint的数据通常保存到hdfs上，放在了高容错文件系统。

# 三.spark运行架构

## 1.Spark运行基本流程 

**Spark运行基本流程参见下面示意图：**

![52262946920](/img/posts/1522629469203.png)

1)      构建Spark Application的运行环境（启动SparkContext），SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源；

2)     资源管理器分配Executor资源并启动Executor，Executor运行情况将随着心跳发送到资源管理器上；

3)    SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task，TaskScheduler将Task发放给Executor运行同时SparkContext将应用程序代码发放给Executor。

4)    Task在Executor上运行，运行完毕释放所有资源。

## 2.Spark运行架构特点

Spark运行架构特点：

- 每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行tasks。

- Spark任务与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了。

- 提交SparkContext的Client应该靠近Worker节点（运行Executor的节点)，最好是在同一个Rack里，因为Spark程序运行过程中SparkContext和Executor之间有大量的信息交换；如果想在远程集群中运行，最好使用RPC将SparkContext提交给集群，不要远离Worker运行SparkContext。

- Task采用了数据本地性和推测执行的优化机制。