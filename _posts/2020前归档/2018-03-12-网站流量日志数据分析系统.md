---
layout:     post
title:      网站流量日志数据分析系统
date:       2017-10-22
author:     yunzhs
header-img: img/Fate of Princess.jpg
catalog: true
tags:
    - bigdata相关
typora-root-url: ..
typora-copy-images-to: ..\img\posts
---

# 一.整体技术流程及框架

## 1.数据处理流程

​	网站流量日志数据分析是一个纯粹的数据分析项目，其整体流程基本上就是依据数据的处理流程进行。有以

下几个大的步骤 ：

- **数据采集**

  ​	数据采集概念，目前行业会有两种解释：一是数据从无到有的过程（web 服务器打印的日志、自定义采

  集的日志等）叫做数据采集；另一方面也有把通过使用 Flume 等工具把数据采集到指定位置的这个过程叫做

  数据采集。关于具体含义要结合语境具体分析，明白语境中具体含义即可。

  ​

- **数据预处理**

   	通过 mapreduce 程序对采集到的原始日志数据进行预处理，比如清洗，格式整理，滤除脏数据等，并且

  梳理成点击流模型数据。

  ​

- **数据入库**

  ​	将预处理之后的数据导入到 HIVE 仓库中相应的库和表中。

  ​

- **数据分析**

  ​	项目的核心内容，即根据需求开发 ETL 分析语句，得出各种统计结果。

  ​

- **数据展现**

  ​	将分析所得数据进行数据可视化，一般通过图表进行展示。

## 2.系统的架构

![1520835382520](/img/posts/1520835382520.png)

![52111490526](/img/posts/1521114905260.png)

相对于传统的 BI 数据处理，流程几乎差不多，但是因为是处理大数据，所以流程中各环节所使用的技术则跟传统 BI 完全不同：

**数据采集**：定制开发采集程序，或使用开源框架 Flume

**数据预处理**：定制开发 mapreduce 程序运行于 hadoop 集群

**数据仓库技术**：基于 hadoop 之上的 Hive

**数据导出**：基于 hadoop 的 sqoop 数据导入导出工具

**数据可视化**：定制开发 web 程序(echarts)

**整个过程的流程调度**：hadoop 生态圈中的 azkaban 工具



​	系统的数据分析不是一次性的，而是按照一定的时间频率反复计算，因而整个处理链条中的各个环节需要按

照一定的先后依赖关系紧密衔接，即涉及到大量任务单元的管理调度，所以，项目中需要添加一个任务调度模块。

# 二.模块开发---- 数据采集

## 1.需求分析

​	在网站 web 流量日志分析这种场景中，对数据采集部分的可靠性、容错能力要求通常不会非常严苛，因此使

用通用的 flume 日志采集框架完全可以满足需求。

## 2.Flume 日志采集系统

### 2.1Flume 采集

Flume 采集系统的搭建相对简单：

1、在服务器上部署 agent 节点，修改配置文件

2、启动 agent 节点，将采集到的数据汇聚到指定的 HDFS 目录中

针对 nginx 日志生成场景，如果通过 flume（1.6）收集，无论是 Spooling DirectorySource 和 Exec Source 均不能满足动态实时收集的需求，在当前 flume1.7 稳定版本中，提供了一个非常好用的 TaildirSource，使用这个 source，可以监控一个目录，并且使用正则表达式匹配该目录中的文件名进行实时收集。

核心配置如下：

```
a1.sources = r1
a1.sources.r1.type = TAILDIR
a1.sources.r1.channels = c1
a1.sources.r1.positionFile = /var/log/flume/taildir_position.json
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.filegroups.f1 = /var/log/test1/example.log
a1.sources.r1.filegroups.f2 = /var/log/test2/.*log.*
```

filegroups:指定 filegroups，可以有多个，以空格分隔；（TailSource 可以同时监控tail 多个目录中的文件）

positionFile:配置检查点文件的路径，检查点文件会以 json 格式保存已经 tail 文件的位置，解决了断点不能续传的缺陷。

filegroups.<filegroupName>：配置每个 filegroup 的文件绝对路径，文件名可以用正则表达式匹配

通过以上配置，就可以监控文件内容的增加和文件的增加。产生和所配置的文件名正则表达式不匹配的文件，则不会被 tail。

### 2.2数据内容样例

```
58.215.204.118 - - [18/Sep/2013:06:51:35 +0000] "GET /wp-includes/js/jquery/jquery.js?ver=1.10.2 HTTP/1.1"
304 0 "http://blog.fens.me/nodejs-socketio-chat/" "Mozilla/5.0 (Windows NT 5.1; rv:23.0) Gecko/20100101
Firefox/23.0"
```

字段解析：
1、访客 ip 地址： 58.215.204.118

2、访客用户信息： - -

3、请求时间：[18/Sep/2013:06:51:35 +0000]

4、请求方式：GET

5、请求的 url：/wp-includes/js/jquery/jquery.js?ver=1.10.2

6、请求所用协议：HTTP/1.1

7、响应码：304

8、返回的数据流量：0

9、访客的来源 url：http://blog.fens.me/nodejs-socketio-chat/

10、访客所用浏览器：Mozilla/5.0 (Windows NT 5.1; rv:23.0) Gecko/20100101Firefox/23.0

# 三.模块开发---- 数据预处理

## 1.主要目的

过滤“不合规”数据，清洗无意义的数据

格式转换和规整

根据后续的统计需求，过滤分离出各种不同主题(不同栏目 path)的基础数据。

![1520853886883](/img/posts/1520853886883.png)

## 2.实现方式

开发一个 mr 程序 WeblogPreProcess( 内容太长，见工程代码 )

```
public class WeblogPreProcess {
static class WeblogPreProcessMapper extends Mapper<LongWritable, Text, Text, NullWritable> {
Text k = new Text();
NullWritable v = NullWritable.get();
@Override
protected void map(LongWritable key, Text value, Context context) throws IOException,
InterruptedException {
String line = value.toString();
WebLogBean webLogBean = WebLogParser.parser(line);
// WebLogBean productWebLog = WebLogParser.parser2(line);
// WebLogBean bbsWebLog = WebLogParser.parser3(line);
// WebLogBean cuxiaoBean = WebLogParser.parser4(line);
if (!webLogBean.isValid())
return;
k.set(webLogBean.toString());
context.write(k, v);
// k.set(productWebLog);
// context.write(k, v);
}
}
public static void main(String[] args) throws Exception {
Configuration conf = new Configuration();
Job job = Job.getInstance(conf);
job.setJarByClass(WeblogPreProcess.class);
job.setMapperClass(WeblogPreProcessMapper.class);
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(NullWritable.class);
FileInputFormat.setInputPaths(job, new Path(args[0]));
FileOutputFormat.setOutputPath(job, new Path(args[1]));
job.waitForCompletion(true);
}
}
```

hadoop jar weblog.jar cn.itcast.bigdata.hive.mr.WeblogPreProcess /weblog/input /weblog/preout

## 3.点击流模型数据梳理

​	由于大量的指标统计从点击流模型中更容易得出，所以在预处理阶段，可以使用 mr 程序来生成点击流模型的数据。

### 3.1点击流模型 pageviews 表

Pageviews 表模型数据生成, 详细见：ClickStreamPageView.java

### 3.2.点击流模型 visit信息表

用 MR 程序从 pageviews 数据中，梳理出每一次 visit 的起止时间、页面信息详细代码见工程：ClickStreamVisit.java

**具体代码见另一篇文章**

# 四.模块开发----ETL

## 1.ETL简介

​	ETL=extract(抽取),transfer(转换),load(加载),就是从各个数据源提取数据，对数据进行转换，并最终加载填充

数据到数据仓库维度建模后的表中。只有当这些维度/事实表被填充好，ETL工作才算完成。

​	本项目的数据分析过程在 hadoop 集群上实现，主要应用 hive 数据仓库工具，因此，采集并经过预处理后的

数据，需要加载到 hive 数据仓库中，以进行后续的分析过程。

## 2.创建ODS层数据表

### 2.1原始数据表

对应mr清洗完之后的数据，而不是原始日志数据	

```
drop table if exists ods_weblog_origin;
create table ods_weblog_origin(
valid string,
remote_addr string,
remote_user string,
time_local string,
request string,
status string,
body_bytes_sent string,
http_referer string,
http_user_agent string)
partitioned by (datestr string)
row format delimited
fields terminated by '\001';
```

### 2.2点击流pageview表

```
drop table if exists ods_click_pageviews;
create table ods_click_pageviews(
session string,
remote_addr string,
remote_user string,
time_local string,
request string,
visit_step string,
page_staylong string,
http_referer string,
http_user_agent string,
body_bytes_sent string,
status string)
partitioned by (datestr string)
row format delimited
fields terminated by '\001';
```

### 2.3点击流visit表

```
drop table if exists ods_click_stream_visit;
create table ods_click_stream_visit(
session     string,
remote_addr string,
inTime      string,
outTime     string,
inPage      string,
outPage     string,
referal     string,
pageVisits  int)
partitioned by (datestr string)
row format delimited
fields terminated by '\001';
```

### 2.4维度表

```
drop table if exists t_dim_time;
create table t_dim_time(date_key int,year string,month string,day string,hour string) row format delimited fields terminated by ',';
```



## 3.导入ODS层数据

**导入清洗结果数据到贴源数据表ods_weblog_origin**

````
load data  local inpath '/export/data/bigdata/part-m-00000' overwrite into table ods_weblog_origin partition(datestr='20130918');
````

**导入点击流模型pageviews数据到ods_click_pageviews表**

```
load data local inpath '/export/data/bigdata/part-r-00000' overwrite into table ods_click_pageviews partition(datestr='20130918');
```

**导入点击流模型visit数据到ods_click_stream_visit表**

```
load data local inpath '/export/data/bigdata/part-r-00000.0' overwrite into table ods_click_stream_visit partition(datestr='20130918');
```

**时间维度表数据导入**

```
load data inpath '/weblog/dim_time' overwrite into table t_dim_time;
```

## 4.生成ODS层明细宽表

### 4.1需求实现

​	整个数据分析的过程是按照数据仓库的层次分层进行的，总体来说，是从ODS 原始数据中整理出一些中间表

（比如，为后续分析方便，将原始数据中的时间、url 等非结构化数据作结构化抽取，将各种字段信息进行细化，

形成明细表），然后再在中间表的基础之上统计出各种指标数据。

### 4.2ETL实现

建明细表 ods_weblog_detail:

```
drop table ods_weblog_detail;
create table ods_weblog_detail(
valid string, --有效标识
remote_addr string, --来源 IP
remote_user string, --用户标识
time_local string, --访问完整时间
daystr string, --访问日期
timestr string, --访问时间
month string, --访问月
day string, --访问日
hour string, --访问时
request string, --请求的 url
status string, --响应码
body_bytes_sent string, --传输字节数
http_referer string, --来源 url
ref_host string, --来源的 host
ref_path string, --来源的路径
ref_query string, --来源参数 query
ref_query_id string, --来源参数 query 的值
http_user_agent string --客户终端标识
)
partitioned by(datestr string);
```

**通过查询插入数据到明细宽表 ods_weblog_detail 中**

1、 抽取 refer_url 到中间表 t_ods_tmp_referurl也就是将来访 url 分离出 host path query query id

```
drop table if exists t_ods_tmp_referurl;
create table t_ods_tmp_referurl as
SELECT a.*,b.*
FROM ods_weblog_origin a 
LATERAL VIEW parse_url_tuple(regexp_replace(http_referer, "\"", ""), 'HOST', 'PATH','QUERY', 'QUERY:id') b as host, path, query, query_id; 
```

LATERAL VIEW parse_url_tuple(regexp_replace(http_referer, "\"", ""), 'HOST', 'PATH','QUERY', 'QUERY:id') b as host, path, query, query_id; 

lateral view 一般后面跟可以把一个字段变成多个字段的函数  把新增的字段作为虚拟的字段**拼在**LATERAL VIEW  左边的表上

parse_url_tuple (**hive自带的函数**) 传入标准的urL --->host path query queryid

regexp_replace(http_referer, "\"", "")将"替换为空

**抽取转换time_local字段到中间表明细表 t_ods_tmp_detail**

```
drop table if exists t_ods_tmp_detail;
create table t_ods_tmp_detail as 
select b.*,substring(time_local,0,10) as daystr,
substring(time_local,12) as tmstr,
substring(time_local,6,2) as month,
substring(time_local,9,2) as day,
substring(time_local,11,3) as hour
From t_ods_tmp_referurl b;
```

最后用的是这个

```
insert into table ods_weblog_detail partition(datestr='20130918')
select c.valid,c.remote_addr,c.remote_user,c.time_local,
substring(c.time_local,0,10) as daystr,
substring(c.time_local,12) as tmstr,
substring(c.time_local,6,2) as month,
substring(c.time_local,9,2) as day,
substring(c.time_local,11,3) as hour,
c.request,c.status,c.body_bytes_sent,c.http_referer,c.ref_host,c.ref_path,c.ref_query,c.ref_query_id,c.http_user_agent
from
(SELECT 
a.valid,a.remote_addr,a.remote_user,a.time_local,
a.request,a.status,a.body_bytes_sent,a.http_referer,a.http_user_agent,b.ref_host,b.ref_path,b.ref_query,b.ref_query_id 
FROM ods_weblog_origin a LATERAL VIEW parse_url_tuple(regexp_replace(http_referer, "\"", ""), 'HOST', 'PATH','QUERY', 'QUERY:id') b as ref_host, ref_path, ref_query, ref_query_id) c;
```

# 四.模块开发---- 统计分析

​	数据仓库建设好以后，用户就可以编写 Hive SQL 语句对其进行访问并对其中数据进行分析。
​	在实际生产中，究竟需要哪些统计指标通常由数据需求相关部门人员提出，而且会不断有新的统计需求产生，以下为网站流量分析中的一些典型指标示例。
​	注：每一种统计指标都可以跟各维度表进行钻取。

## 1.流量分析

### 1.1多维度统计 PV 总量

**按时间维度**

```
-- 计算每小时 pvs ，注意 gruop by 语法
select count(*) as pvs,month,day,hour from ods_weblog_detail group by month,day,hour;
```

方式一：直接在 ods_weblog_detail 单表上进行查询

```

-- 计算该处理批次（一天）中的各小时 pvs
drop table dw_pvs_everyhour_oneday;
create table dw_pvs_everyhour_oneday(month string,day string,hour string,pvs bigint) partitioned by(datestr
string);
insert into table dw_pvs_everyhour_oneday partition(datestr='20130918')
select a.month as month,a.day as day,a.hour as hour,count(*) as pvs from ods_weblog_detail a
where a.datestr='20130918' group by a.month,a.day,a.hour;
-- 计算每天的 pvs
drop table dw_pvs_everyday;
create table dw_pvs_everyday(pvs bigint,month string,day string);
insert into table dw_pvs_everyday
select count(*) as pvs,a.month as month,a.day as day from ods_weblog_detail a
group by a.month,a.day;
```

方式 二：与时间维表关联查询

````
-- 维度：日
drop table dw_pvs_everyday;
create table dw_pvs_everyday(pvs bigint,month string,day string);
insert into table dw_pvs_everyday
select count(*) as pvs,a.month as month,a.day as day from (select distinct month, day from t_dim_time) a
join ods_weblog_detail b
on a.month=b.month and a.day=b.day
group by a.month,a.day;
-- 维度：月
drop table dw_pvs_everymonth;
create table dw_pvs_everymonth (pvs bigint,month string);
insert into table dw_pvs_everymonth
select count(*) as pvs,a.month from (select distinct month from t_dim_time) a
join ods_weblog_detail b on a.month=b.month group by a.month;
-- 另外，也可以直接利用之前的计算结果。比如从之前算好的小时结果中统计每一天的
Insert into table dw_pvs_everyday
Select sum(pvs) as pvs,month,day from dw_pvs_everyhour_oneday group by month,day having day='18';
````

### 1.2人均浏览量

需求描述：统计今日所有来访者平均请求的页面数。
人均浏览量也称作人均浏览页数，该指标可以说明网站对用户的粘性。
人均页面浏览量表示用户某一时段平均浏览页面的次数。
计算方式：总页面请求数/去重总人数
remote_addr表示不同的用户。可以先统计出不同 remote_addr 的 pv量，然后累加（sum）
所有 pv 作为总的页面请求数，再 count 所有 remote_addr 作为总的去重总人数。

```
-- 总页面请求数 / 去重总人数
drop table dw_avgpv_user_everyday;
create table dw_avgpv_user_everyday(
day string,
avgpv string);
insert into table dw_avgpv_user_everyday
select '20130918',sum(b.pvs)/count(b.remote_addr) from
(select remote_addr,count(1) as pvs from ods_weblog_detail where datestr='20130918' group by remote_addr) b;
```

### 1.3 统计 pv 总量最大的来源 TOPN ( 分组 TOP)

统计每小时各来访url产生的pv量，查询结果存入：( "dw_pvs_referer_everyhour" )

```
drop table dw_pvs_referer_everyhour;
create table dw_pvs_referer_everyhour(referer_url string,referer_host string,month string,day string,hour string,pv_referer_cnt bigint) partitioned by(datestr string);

insert into table dw_pvs_referer_everyhour partition(datestr='20130918')
select http_referer,ref_host,month,day,hour,count(1) as pv_referer_cnt
from ods_weblog_detail 
group by http_referer,ref_host,month,day,hour 
having ref_host is not null
order by hour asc,day asc,month asc,pv_referer_cnt desc;
```



需求描述：统计每小时各来访 host 的产生的 pvs 数最多的前 N 个（topN）。
row_number() 函数

- 语法：row_number() over (partition by xxx order by xxx) rank，rank 为分

组的别名，相当于新增一个字段为 rank。

- partition by 用于分组，比方说依照 sex 字段分组
- order by 用于分组内排序，比方说依照 sex 分组，组内按照 age 排序
- 排好序之后，为每个分组内每一条分组记录从 1 开始返回一个数字
- 取组内某个数据，可以使用 where 表名.rank>x 之类的语法去取以下语句对每个小时内的来访 host 次数倒序排序标号:

--row_number函数

```
select ref_host,ref_host_cnts,concat(month,day,hour),
row_number() over (partition by concat(month,day,hour) order by
ref_host_cnts desc) as od from dw_pvs_refererhost_everyhour;
```

--综上可以得出

```
drop table dw_pvs_refhost_topn_everyhour;

create table dw_pvs_refhost_topn_everyhour(

hour string,

toporder string,

ref_host string,

ref_host_cnts string

)partitioned by(datestr string);

insert into table dw_pvs_refhost_topn_everyhour partition(datestr='20130918')

select t.hour,t.od,t.ref_host,t.ref_host_cnts from

 (select ref_host,ref_host_cnts,concat(month,day,hour) as hour,

row_number() over (partition by concat(month,day,hour) order by ref_host_cnts desc) as od 

from dw_pvs_refererhost_everyhour) t where od<=3;

```

## 2.受访分析 （从页面的角度分析）

### 2.1各页面访问统计

​	主要是针对数据中的 request 进行统计分析，比如各页面 PV ，各页面 UV 等。以上指标无非就是根据页面的字段 group by。例如：

```
统计各页面 pv
select request as request,count(request) as request_counts from
ods_weblog_detail group by request having request is not null order by request_counts desc limit 20;
```

### 2.1热门页面统计

```
-- 统计每日最热门的页面 top10
drop table dw_hotpages_everyday;
create table dw_hotpages_everyday(day string,url string,pvs string);

insert into table dw_hotpages_everyday
select '20130918',a.request,a.request_counts from
(select request as request,count(request) as request_counts from ods_weblog_detail where datestr='20130918'
group by request having request is not null) a
order by a.request_counts desc limit 10;
```

## 3.访客分析

### 3.1独立访客

需求描述：按照时间维度比如小时来统计独立访客及其产生的 pv。

对于独立访客的识别，如果在原始日志中有用户标识，则根据用户标识即很好实现;此处，由于

原始日志中并没有用户标识，以访客 IP 来模拟，技术上是一样的，只是精确度相对较低。

```
-- 时间维度：时
drop table dw_user_dstc_ip_h;
create table dw_user_dstc_ip_h(
remote_addr string,
pvs bigint,
hour string);

insert into table dw_user_dstc_ip_h
select remote_addr,count(1) as pvs,concat(month,day,hour) as hour
from ods_weblog_detail
Where datestr='20130918'
group by concat(month,day,hour),remote_addr;
在此结果表之上，可以进一步统计，如每小时独立访客总数：
select count(1) as dstc_ip_cnts,hour from dw_user_dstc_ip_h group by hour;
```

```
-- 时间维度：日
select remote_addr,count(1) as counts,concat(month,day) as day
from ods_weblog_detail
Where datestr='20130918'
group by concat(month,day),remote_addr;
```

```
-- 时间维度：月
select remote_addr,count(1) as counts,month
from ods_weblog_detail
group by month,remote_addr;
```

### 3.2每日新访客

需求：将每天的新访客统计出来。

实现思路：创建一个去重访客累积表，然后将每日访客对比累积表。

![1521019646437](/img/posts/1521019646437.png)

```
-- 历日去重访客累积表
drop table dw_user_dsct_history;
create table dw_user_dsct_history(
day string,
ip string
)
partitioned by(datestr string);

-- 每日新访客表
drop table dw_user_new_d;
create table dw_user_new_d (
day string,
ip string
)
partitioned by(datestr string);

-- 每日新用户插入新访客表
insert into table dw_user_new_d partition(datestr='20130918')
select tmp.day as day,tmp.today_addr as new_ip from
(
select today.day as day,today.remote_addr as today_addr,old.ip as old_addr
from
(select distinct remote_addr as remote_addr,"20130918" as day from ods_weblog_detail where
datestr="20130918") today
left outer join
dw_user_dsct_history old
on today.remote_addr=old.ip
) tmp
where tmp.old_addr is null;

-- 每日新用户追加到累计表
insert into table dw_user_dsct_history partition(datestr='20130918')
select day,ip from dw_user_new_d where datestr='20130918';
```

验证查看：

```
select count(distinct remote_addr) from ods_weblog_detail;
select count(1) from dw_user_dsct_history where datestr='20130918';
select count(1) from dw_user_new_d where datestr='20130918';
```

## 4.访客 Visit 分析（点击流模型）

### 4.1 回头单次访客统计

需求：查询今日所有回头访客及其访问次数。

实现思路：上表中出现次数>1 的访客，即回头访客；反之，则为单次访客。

```
drop table dw_user_returning;
create table dw_user_returning(
day string,
remote_addr string,
acc_cnt string)
partitioned by (datestr string);
insert overwrite table dw_user_returning partition(datestr='20130918')
select tmp.day,tmp.remote_addr,tmp.acc_cnt
from
(select '20130918' as day,remote_addr,count(session) as acc_cnt from ods_click_stream_visit group by
remote_addr) tmp
where tmp.acc_cnt>1;
```

### 4.2人均访问频次

需求：统计出每天所有用户访问网站的平均次数（visit）
总 visit 数/去重总用户数

```
select sum(pagevisits)/count(distinct remote_addr) from ods_click_stream_visit where datestr='20130918';
```



# 五.模块开发---- 结果导出

详见Apache Sqoop使用

# 六.模块开发---- 工作流调度

详见 azkaban 的使用

# 七.模块开发---- 数据可视化

详见Echarts使用