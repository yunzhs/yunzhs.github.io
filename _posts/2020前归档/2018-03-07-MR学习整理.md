---
layout:     post
title:      MR学习整理
date:       2017-09-17
author:     yunzhs
header-img: img/Fate of Princess.jpg
catalog: true
tags:
    - hadoop
typora-root-url: ..
typora-copy-images-to: ..\img\posts
---

# 一.MapReduce入门

## 1. MapReduce 计算模型介绍

### 1.1 理解 MapReduce 思想

​	MapReduce 的思想核心是“ 分而治之”，适用于大量复杂的任务处理场景（大规模数据处理场景）。

​	Map 负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可

以并行计算，彼此间几乎没有依赖关系。

​	Reduce 负责“合”，即对 map 阶段的结果进行全局汇总。

下图完美的诠释的了MR的思想

![1520399592549](/img/posts/1520399592549.png)

## 1.2 Hadoop MapReduce 设计构思

​	MapReduce 是一个分布式运算程序的编程框架，核心功能是**将用户编写的业务逻辑代码和自带默认组件整合**

**成一个完整的分布式运算程序**，并发运行在Hadoop 集群上。

​	表现形式就是有个输入（input），MapReduce 操作这个输入（input），通过本身定义好的计算模型，得到

一个输出（output）。

​	Hadoop MapReduce 构思体现在如下的三个方面：

-  如何对付大数据处理：分而治之

  ​	对相互间不具有计算依赖关系的大数据，实现并行最自然的办法就是采取分而治之的策略。并行计算的第

  一个重要问题是如何划分计算任务或者计算数据以便对划分的子任务或数据块同时进行计算。不可分拆的计算

  任务或相互间有依赖关系的数据无法进行并行计算！

-  构建抽象模型：Map 和 Reduce

  ​	MapReduce 借鉴了函数式语言中的思想，用 Map 和 Reduce 两个函数提供了高层的并行编程抽象模型。
  ​	Map: 对一组数据元素进行某种重复式的处理；
  ​	Reduce: 对 Map 的中间结果进行某种进一步的结果整理。
  ​	MapReduce 中定义了如下的 Map 和 Reduce 两个抽象的编程接口，由用户去编程实现:
  ​	map: (k1; v1) → [(k2; v2)]
  ​	reduce: (k2; [v2]) → [(k3; v3)]
  ​	Map 和 Reduce 为程序员提供了一个清晰的操作接口抽象描述。通过以上两个编程接口，大家可以看出 MapReduce 处理的数据类型是<key,value>键值对。

-  统一构架，隐藏系统层细节

   ​	如何提供统一的计算框架，如果没有统一封装底层细节，那么程序员则需要考虑诸如数据存储、划分、分

   发、结果收集、错误恢复等诸多细节；为此，MapReduce 设计并提供了统一的计算框架，为程序员隐藏了绝

   大多数系统层面的处理细节。

   ​	MapReduce 最大的亮点在于通过抽象模型和计算框架把需要做什么(whatneed to do)与具体怎么做(how 

   to do)分开了，为程序员提供一个抽象和高层的编程接口和框架。程序员仅需要关心其应用层的具体计算问

   题，仅需编写少量的处理应用本身计算问题的程序代码。如何具体完成这个并行计算任务所相关的诸多系统层

   细节被隐藏起来,交给计算框架去处理：从分布代码的执行，到大到数千小到单个节点集群的自动调度使用。

### 1.3 MapReduce

一个完整的 mapreduce 程序在分布式运行时有三类实例进程：

1、MRAppMaster：负责整个程序的过程调度及状态协调

2、MapTask：负责 map 阶段的整个数据处理流程

3、ReduceTask：负责 reduce 阶段的整个数据处理流程

##  2.MapReduce 编程规范及示例编写

### 2.1编程规范

（1） 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行 mr 程序的客户端)

（2）Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义）

（3）Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义）

（4）Mapper 中的业务逻辑写在 map()方法中

（5）map()方法（maptask 进程）对每一个<K,V>调用一次

（6）Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV

（7）Reducer 的业务逻辑写在 reduce()方法中

（8）Reducetask 进程对每一组相同 k 的<k,v>组调用一次 reduce()方法

（9）用户自定义的 Mapper 和 Reducer 都要继承各自的父类

（10）整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象

### 2.2 WordCount示例编写

**需求:**在一堆给定的文本文件中统计输出每一个单词出现的总次数

**其中**,map(),reduce()都是我们用来实现具体业务逻辑的方法



**(1) 定义一个 mapper 类**

```
import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;


/**
 * 
 * Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>
 * 
 * KEYIN表示map阶段输入的<k,v>中k的类型，在默认的读取数据组件（TextInputFormat）下,该组件一行一行读取待处理数据
 * 把改行的  起始偏移量 作为k,把改行内容作为k  <0,hadoop allen spark><19,itcast hadoop>   因此k :long
 * 
 * VALUEIN表示map阶段输入的<k,v>中v的类型，在默认的读取数据组件（TextInputFormat）下,就是一行行内容  v:String
 * 
 * KEYOUTKEYIN表示map阶段输出的<k,v>中k的类型，在单词统计中  k就表示单词    k:String
 * 
 * VALUEOUT表示map阶段输出的<k,v>中v的类型，在单词统计中，就是单词次数1  v:Int
 * 
 * int String 这些是jdk自带的数据类型，在跨网络传输中需要进行序列化和反序列化操作，hadoop认为jdk序列化机制太累赘 太啰嗦  ，因此hadoop自己实现了一套序列化机制  Writable
 * 
 * int----->intWritable
 * 
 * String------>Text
 * 
 * long-------->longWritable
 * 
 * null------>nullWritable
 * 
 * 
 */
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{
    
    //这个方法就是map阶段具体业务逻辑实现的方法
    //<0,hadoop allen spark><19,itcast hadoop>
    //本方法的生命周期   默认读取数据的组件每读取一行内容 就封装一个kv对  这个一个就调用一次该方法
    //TextInputFormat 读取几行返回几个<k,v> 该方法就调用几次
    @Override
    protected void map(LongWritable key, Text value,
            Mapper<LongWritable, Text, Text, IntWritable>.Context context)
            throws IOException, InterruptedException {
            //<0,hadoop allen spark>
        
            //获取传入的改行内容 转为String类型
            String line = value.toString();  //hadoop allen spark
            
            //把改行内容按照分隔符切成数组
            String[] words = line.split(" ");//[hadoop,allen,spark]
            
            //遍历该数组 出现一个单词 标记为1
            
            for (String word : words) {
                
                //使用框架提供的上下文对象  把map阶段的处理结果写出去  作为reduce阶段的输入
                context.write(new Text(word), new IntWritable(1));   //<hadoop,1> <allen,1> <spark，1>
                
            }
        

    }

}
```

**(2) 定义一个 reducer 类**

```
import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
/**
 * 
 * Reducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT>
 * 
 * KEYIN表示的reduce阶段输入<k，v>中的k类型,在单词统计案例就是单词   Text, 也就是map阶段输出的<k，v>中的k
 * 
 * VALUEIN表示的reduce阶段输入<k，v>中的v类型,在单词统计案例就是单词 次数1  IntWritable, 也就是map阶段输出的<k，v>中的v
 * 
 * KEYOUT表示的reduce阶段输出<k，v>中的k类型,在单词统计案例就是单词   Text
 * 
 * VALUEOUT表示的reduce阶段输出<k，v>中的v类型,在单词统计案例就是单词   总次数  IntWritable
 * 
 */
public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
    
    //该方法就是reduce阶段具体业务逻辑实现的地方
    //<hadoop,1> <allen,1> <spark，1> <allen,1> <allen,1><hadoop,1> 
    
    //reduce把上一阶段的所有数据按照k的字典序排序
    //<allen,1><allen,1><allen,1><hadoop,1><hadoop,1><spark，1>
    //排好序之后，按照k是否相同作为一组   这一组相同的k作为共同的k 这一组所有的v组成迭代器  构成kv对 传入reduce方法    因此也是一组一组调用
    
    //<allen,1><allen,1><allen,1>-------------><allen,[1,1,1]>
    //<hadoop,1><hadoop,1>   -----------------><hadoop,[1,1]>
    
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values,
            Reducer<Text, IntWritable, Text, IntWritable>.Context context)
            throws IOException, InterruptedException {
        
               //定义计数器 用来统计单词的总次数
               int count = 0;
               //遍历单词所有v构成的 Iterable  拿出次数累加成为最终该单词的总次数              
               for (IntWritable value : values) {
                   count +=value.get();
            }
               //通过上下文 把最终结果输出
               context.write(key, new IntWritable(count)); //<allen,3>     
        
    }

}

```

(3) 定义一个主类，用来描述 job 并提交 job

```
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCountRuner {
    
    public static void main(String[] args) throws Exception{
        Configuration conf = new Configuration();
        //通过job构造本次mr程序运行的主类  在当中需要指定一些信息  使用的是哪个map 那个reduce  数据在哪  输出在哪  各个阶段数据类型
        
//        conf.set("mapreduce.framework.name", "local");
        
        Job job = Job.getInstance(conf);
        
        //指定本次mr程序运行的主类
        job.setJarByClass(WordCountRuner.class);
        
        //指定本次mr程序的map类 reduce类
        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);
        
        //指定map阶段输出的数据类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);
        
        //指定reduce阶段输出的数据类型  也就是整个程序最终的输出
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        
        //指定本次mr程序的输入和输出
        FileInputFormat.setInputPaths(job, "D:\\wordcount\\input"); //args 动态传入
        FileOutputFormat.setOutputPath(job, new Path("D:\\wordcount\\output"));
        
//        job.submit();
        //提交并且追踪打印程序执行的状况
        boolean b = job.waitForCompletion(true);
        
        //程序退出状态指定
        System.exit(b? 0 :1);
        
    }

}



```

## 3.MapReduce 程序运行模式

### 3.1 本地运行模式

（1）mapreduce 程序是被提交给 LocalJobRunner 在本地以单进程的形式运行

（2）而处理的数据及输出结果可以在本地文件系统，也可以在 hdfs 上

（3）怎样实现本地运行？写一个程序，不要带集群的配置文件本质是程序的 conf 中是否有 

mapreduce.framework.name=local 以及yarn.resourcemanager.hostname 参数

（4）本地模式非常便于进行业务逻辑的 debug，只要在 eclipse 中打断点即可

**在idea上运行会报一个windows的本地化错误,需要将hadoop.all移入windows\system32中**

### 3.2 集群运行模式

（1）将 mapreduce 程序提交给 yarn 集群，分发到很多的节点上并发执行

（2）处理的数据和输出结果应该位于 hdfs 文件系统

（3）提交集群的实现步骤：将程序打成 JAR 包，然后在集群的任意一个节点上用 hadoop 命令启动
hadoop jar wordcount.jar cn.itcast.bigdata.mrsimple.WordCountDriver args

# 二.深入MapReduce

## 1.MapReduce的输入和输出

​	MapReduce 框架运转在<key,value> 键值对上，也就是说，框架把作业的输入看成是一组

<key,value>键值对，同样也产生一组<key,value>键值对作为作业的输出，这两组键值对可能是

不同的。

​	一个 MapReduce 作业的输入和输出类型如下图所示：可以看出在整个标准的流程中，会有

三组<key,value>

键值对类型的存在。

![1520422239176](/img/posts/1520422239176.png)

## 2.MapReduce 的处理流程解析

### 2.1Mapper 任务执行过程详解

- 第一阶段是把输入目录下文件按照一定的标准逐个进行逻辑切片，形成切片规划。默认情况下，Split size = Block size。每一个切片由一个MapTask 处理。（getSplits）
- 第二阶段是对切片中的数据按照一定的规则解析成<key,value>对。默认规则是把每一行文本内容解析成键值对。key 是每一行的起始位置(单位是字节)，value 是本行的文本内容。（TextInputFormat）
- 第三阶段是调用 Mapper 类中的 map 方法。上阶段中每解析出来的一个<k,v>，调用一次 map 方法。每次调用 map 方法会输出零个或多个键值对。
- 第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。默认是只有一个区。分区的数量就是 Reducer 任务运行的数量。默认只有一个Reducer 任务。
- 第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值对<2,2>、<1,3>、<2,1>，键和值分别是整数。那么排序后的结果是<1,3>、<2,1>、<2,2>。如果有第六阶段，那么进入第六阶段；如果没有，直接输出到文件中。
- 第六阶段是对数据进行局部聚合处理，也就是 combiner 处理。键相等的键值对会调用一次 reduce 方法。经过这一阶段，数据量会减少。 本阶段默认是没有的

### 2.2 Reducer 任务执行过程详解

- 第一阶段是 Reducer 任务会主动从 Mapper 任务复制其输出的键值对。Mapper 任务可能会有很多，因此 Reducer 会复制多个 Mapper 的输出。


- 第二阶段是把复制到 Reducer 本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。


- 第三阶段是对排序后的键值对调用 reduce 方法。键相等的键值对调用一次reduce 方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到 HDFS 文件中


==在整个  MapReduce 程序的开发过程中，我们最大的工作量是覆盖  map 函数和覆盖 reduce 函数。==

## 3.MapReduce的序列化

### 1. 概述

​	序列化（Serialization）是指把结构化对象转化为字节流。

​	反序列化（Deserialization）是序列化的逆过程。把字节流转为结构化对象。

​	当要在进程间传递对象或持久化对象的时候，就需要序列化对象成字节流，反之当要将接收到或从磁盘读取的字节流转换为对象，就要进行反序列化。

​	Java 的序列化（Serializable）是一个重量级序列化框架，一个对象被序列化后，会附带很多额外的信息（各

种校验信息，header，继承体系…），不便于在网络中高效传输；所以，hadoop 自己开发了一套序列化机制（ 

Writable），精简，高效。不用像 java 对象类一样传输多层的父子关系，需要哪个属性就传输哪个属性值，大大

的减少网络传输的开销。

​	Writable是Hadoop的序列化格式，hadoop定义了这样一个Writable接口。一个类要支持可序列化只需实现

这个接口即可。

```
public interface Writable {
 void write(DataOutput out) throws IOException;
 void readFields(DataInput in) throws IOException;
}
```

## 2.Writable 序列化接口

​	如需要将自定义的 bean 放在 key 中传输，则还需要实现 comparable 接口，因为 mapreduce 框中的 shuffle 过程一定会对 key 进行排序,此时，自定义的bean 实现的接口应该是：
public class FlowBean implements WritableComparable<FlowBean>
需要自己实现的方法是：

```
/**
* 反序列化的方法，反序列化时，从流中读取到的各个字段的顺序应该与序列化时
写出去的顺序保持一致
*/
@Override
public void readFields(DataInput in) throws IOException {
upflow = in.readLong();
dflow = in.readLong();
sumflow = in.readLong();
}
/**
* 序列化的方法
*/
@Override
public void write(DataOutput out) throws IOException {
out.writeLong(upflow);
out.writeLong(dflow);
out.writeLong(sumflow);
}
@Override
public int compareTo(FlowBean o) {
//实现按照 sumflow 的大小倒序排序
return sumflow>o.getSumflow()?-1:1;
}
```

## 4.MapReduce的排序初步

### 1.需求分析

​	在得出统计每一个用户（手机号）所耗费的总上行流量、下行流量，总流量结果的基础之上再加一个需求：

将统计结果按照总流量倒序排序。

### 2.基本思路

​	实现自定义的 bean 来封装流量信息，并将  bean 作为  map 输出的 key 来传输

​	MR 程序在处理数据的过程中会对数据排序(map 输出的 kv 对传输到 reduce之前，会排序)，排序的依据是 

map 输出的 key。所以，我们如果要实现自己需要的排序规则，则可以考虑将排序因素放到 key 中，让 key 实现

接口：WritableComparable，然后重写 key 的 compareTo 方法。

### 3.实现代码

**自定义的bean:**

```
public class FlowBean implements WritableComparable<FlowBean>{
    
    private Long upFlow;
    
    private Long downFlow;
    
    private Long sumFlow;
    
    

    public FlowBean() {
        
    }

    public FlowBean(Long upFlow, Long downFlow, Long sumFlow) {
        super();
        this.upFlow = upFlow;
        this.downFlow = downFlow;
        this.sumFlow = sumFlow;
    }
    
    public FlowBean(Long upFlow, Long downFlow) {
        super();
        this.upFlow = upFlow;
        this.downFlow = downFlow;
        this.sumFlow = upFlow+downFlow;
    }
    
    
    public void set(Long upFlow, Long downFlow) {
        this.upFlow = upFlow;
        this.downFlow = downFlow;
        this.sumFlow = upFlow+downFlow;
    }

    public Long getUpFlow() {
        return upFlow;
    }

    public void setUpFlow(Long upFlow) {
        this.upFlow = upFlow;
    }

    public Long getDownFlow() {
        return downFlow;
    }

    public void setDownFlow(Long downFlow) {
        this.downFlow = downFlow;
    }

    public Long getSumFlow() {
        return sumFlow;
    }

    public void setSumFlow(Long sumFlow) {
        this.sumFlow = sumFlow;
    }

    @Override
    public String toString() {
        return upFlow+"\t"+downFlow+"\t"+sumFlow;
    }

    //这是序列化方法
    @Override
    public void write(DataOutput out) throws IOException {
        out.writeLong(upFlow);
        out.writeLong(downFlow);
        out.writeLong(sumFlow);
        
    }

    //这是反序列化的方法
    //注意 反序列化的顺序 先序列化先出来
    @Override
    public void readFields(DataInput in) throws IOException {
       this.upFlow =  in.readLong();
       this.downFlow =  in.readLong();
       this.sumFlow =  in.readLong();
    }

    //这里是对象比较大小的指定地方
    @Override
    public int compareTo(FlowBean o) {
        //人为修改排序规则  虽然我比你大  但是返回比较结果显示我比你小  从而形成倒序
        return this.getSumFlow()>o.getSumFlow()?-1:1;
    }
    

}
```

运行主类

mapper

```
public class FlowSumMapper extends Mapper<LongWritable, Text, Text, FlowBean> {

    Text k = new Text();

    FlowBean v = new FlowBean();

    @Override
    protected void map(LongWritable key, Text value,Context context)
            throws IOException, InterruptedException {
        String line = value.toString();

        String[] fields = line.split("\t");

        String phoneNum = fields[1];
        long upFlow = Long.parseLong(fields[fields.length - 3]);
        long downFlow = Long.parseLong(fields[fields.length - 2]);

        k.set(phoneNum);
        v.set(upFlow, downFlow);
        context.write(k, v);
    }

}

```

reduce

```
public class FLowSumReducer extends Reducer<Text, FlowBean, Text, FlowBean> {
    
    FlowBean v = new FlowBean();

    @Override
    protected void reduce(Text key, Iterable<FlowBean> values,
            Reducer<Text, FlowBean, Text, FlowBean>.Context context)
            throws IOException, InterruptedException {

        long upSumFlow = 0;
        long downSumFLow = 0;

        for (FlowBean flowBean : values) {
            upSumFlow += flowBean.getUpFlow();
            downSumFLow += flowBean.getDownFlow();
        }
        
        v.set(upSumFlow, downSumFLow);
        context.write(key, v);

    }

}
```

driver

```
public class FLowSumDriver {
    public static void main(String[] args) throws Exception{
        Configuration conf = new Configuration();
        //通过job构造本次mr程序运行的主类  在当中需要指定一些信息  使用的是哪个map 那个reduce  数据在哪  输出在哪  各个阶段数据类型
        
//        conf.set("mapreduce.framework.name", "local");
        
        Job job = Job.getInstance(conf);
        
        //指定本次mr程序运行的主类
        job.setJarByClass(FLowSumDriver.class);
        
        //指定本次mr程序的map类 reduce类
        job.setMapperClass(FlowSumMapper.class);
        job.setReducerClass(FLowSumReducer.class);
        
        //指定map阶段输出的数据类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(FlowBean.class);
        
        //指定reduce阶段输出的数据类型  也就是整个程序最终的输出
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(FlowBean.class);
        
        
        //指定本次mr程序的输入和输出
        FileInputFormat.setInputPaths(job, "D:\\flowsum\\input"); //args 动态传入
        FileOutputFormat.setOutputPath(job, new Path("D:\\flowsum\\output"));
        
//        job.submit();
        //提交并且追踪打印程序执行的状况
        boolean b = job.waitForCompletion(true);
        
        //程序退出状态指定
        System.exit(b? 0 :1);
        
    }

}
```

## 5.Mapreduce 的分区—Partitioner

### 1.需求分析

​	将流量汇总统计结果按照手机归属地不同省份输出到不同文件中。

### 2. 思路

​	Mapreduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask。

​	默认的分发规则为：根据 key 的 hashcode%reducetask 数来分发

​	所以：如果要按照我们自己的需求进行分组，则需要改写数据分发（分组）组件 Partitioner，自定义一个 

CustomPartitioner 继承抽象类：Partitioner，然后在job 对象中，设置自定义 partitioner： 

job.setPartitionerClass(CustomPartitioner.class

### 3.实现

```
public class ProvincePartitioner extends Partitioner<Text, FlowBean>
public static HashMap<String, Integer> provinceMap = new HashMap<String, Integer>();
static{
provinceMap.put("134", 0);
provinceMap.put("135", 1);
provinceMap.put("136", 2);
provinceMap.put("137", 3);
provinceMap.put("138", 4);
}
@Override
public int getPartition(Text key, FlowBean value, int numPartitions) {
Integer code = provinceMap.get(key.toString().substring(0, 3));
if (code != null) {
return code;
}
return 5;
}
}
```

​	

三合一代码

```
public class FlowSumProvince {
    
 public static class FlowSumProvinceMapper extends Mapper<LongWritable, Text, Text, FlowBean>{
        
     Text k = new Text();
     FlowBean  v = new FlowBean();
     
     @Override
     protected void map(LongWritable key, Text value,Context context)
             throws IOException, InterruptedException {
             //拿取一行文本转为String
             String line = value.toString();
             //按照分隔符\t进行分割
             String[] fileds = line.split("\t");
             //获取用户手机号
             String phoneNum = fileds[1];
             
             long upFlow = Long.parseLong(fileds[fileds.length-3]);
             long downFlow = Long.parseLong(fileds[fileds.length-2]);
             
             k.set(phoneNum);
             v.set(upFlow, downFlow);
             
             context.write(k,v);
                
        }
        
    }
    
    
    public static class FlowSumProvinceReducer extends Reducer<Text, FlowBean, Text, FlowBean>{
        
        FlowBean  v  = new FlowBean(); 
        
        @Override
        protected void reduce(Text key, Iterable<FlowBean> flowBeans,Context context) throws IOException, InterruptedException {
            
            long upFlowCount = 0;
            long downFlowCount = 0;
            
            for (FlowBean flowBean : flowBeans) {
                
                upFlowCount += flowBean.getUpFlow();
                
                downFlowCount += flowBean.getDownFlow();
                
            }
            v.set(upFlowCount, downFlowCount);
            
            context.write(key, v);
    }
    
    
    public static void main(String[] args) throws Exception{
        

        Configuration conf = new Configuration();

        Job job = Job.getInstance(conf);

        //指定我这个 job 所在的 jar包位置
        job.setJarByClass(FlowSumProvince.class);
        
        //指定我们使用的Mapper是那个类  reducer是哪个类
        job.setMapperClass(FlowSumProvinceMapper.class);
        job.setReducerClass(FlowSumProvinceReducer.class);
        
        // 设置我们的业务逻辑 Mapper 类的输出 key 和 value 的数据类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(FlowBean.class);
        
        // 设置我们的业务逻辑 Reducer 类的输出 key 和 value 的数据类型
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(FlowBean.class);
        
        
        //这里设置运行reduceTask的个数
        //如果ReduceTask个数  >  分区的个数   可以执行 多余的reducetask因为没有数据分给他 所有重在参与  浪费资源
        //如果ReduceTask个数  <  分区的个数  程序报错  Illegal partition
        job.setNumReduceTasks(3);
        
        
        //这里指定使用我们自定义的分区组件
        //不写就是默认
        job.setPartitionerClass(ProvincePartitioner.class);
        
        
        FileInputFormat.setInputPaths(job, new Path("D:\\flowsum\\input"));
        // 指定处理完成之后的结果所保存的位置
        FileOutputFormat.setOutputPath(job, new Path("D:\\flowsum\\outputProvince"));
        
        boolean res = job.waitForCompletion(true);
        System.exit(res ? 0 : 1);
        
    }

 }
}
```



## 6.Mapreduce的combiner

​	每一个 map 都可能会产生大量的本地输出，Combiner 的作用就是对 map 端的输出先做一

次合并，以减少在 map 和 reduce 节点之间的数据传输量，以提高网络 IO 性能，是

MapReduce 的一种优化手段之一。

- combiner 是 MR 程序中 Mapper 和 Reducer 之外的一种组件

- combiner 组件的父类就是 Reducer

- combiner 和 reducer 的区别在于运行的位置：

  Combiner 是在每一个 maptask 所在的节点运行
  Reducer 是接收全局所有 Mapper 的输出结果；

- combiner 的意义就是对每一个 maptask 的输出进行局部汇总，以减小网络传输量


- 具体实现步骤：

  1、自定义一个 combiner 继承 Reducer，重写 reduce 方法
  2、在 job 中设置： job.setCombinerClass(CustomCombiner.class)

- combiner 能够应用的前提是不能影响最终的业务逻辑，而且，combiner 的

  输出 kv 应该跟 reducer 的输入 kv 类型要对应起来