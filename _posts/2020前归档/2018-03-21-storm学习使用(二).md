---
layout:     post
title:      storm的学习使用(二)
date:       2018-1-20
author:     yunzhs
header-img: img/Fate of Princess.jpg
catalog: true
tags:
    - storm
typora-root-url: ..
typora-copy-images-to: ..\img\posts
---

## 一.storm提交任务的流程

### 1.nibums启动之后,接受客户端提交任务

 命令格式：storm jar xxx.jar   xxx驱动类  参数

会将用户的jar上传到 nimbus物理节点的 /export/data/storm/workdir/nimbus/inbox目录下。并且改名，改名的规则是添加了一个UUID字符串。

在nimbus物理节点的
/export/data/storm/workdir/nimbus/stormdist目录下。有当前正在运行的topology的jar包和配置文件，序列化对象文件。

### 2.分配任务信息到zookeeper

​	nimbus接受到任务之后，会将任务进行分配，分配会产生一个assignment对象，该对象会保存到zk中，目录

是/storm/assignments ，该目录只保存正在运行的topology任务。

### 3.supervisor通过watch机制从zk上拉取任务

​	并根据具体的任务信息,启动自己的worker,并分配一个端口,work启动后,连接zk,拉取任务

### 4.worker根据任务类型，分别执行spout任务或者bolt任务

​	spout的声明周期是：open、nextTuple、outPutFiled

​      bolt的生命周期是：prepare、execute(tuple)、outPutFiled

==因此在storm的代码当中所有的java类都要实现序列化serialiazble 接口才可以走网络传输==



## 二.消息不丢失机制

### 1.ack是什么

​	通过Ack机制，spout发送出去的每一条消息，都可以确定是被成功处理或失败处理， 从而可以让开发者采取

动作。比如在Meta中，成功被处理，即可更新偏移量，当失败时，重复发送数据。

​	因此，通过Ack机制，很容易做到保证所有数据均被处理，一条都不漏。

​	另外需要注意的，当spout触发fail动作时，不会自动重发失败的tuple，需要spout自己重新获取数据，手动

重新再发送一次

ack机制即:

spout发送的每一条消息

- 在规定的时间内，spout收到Acker的ack响应，即认为该tuple 被后续bolt成功处理
- 在规定的时间内，没有收到Acker的ack响应tuple，就触发fail动作，即认为该tuple处理失败
- 或者收到Acker发送的fail响应tuple，也认为失败，触发fail动作

这个timeout时间可以通过Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS来设定。Timeout的默认时长为30秒

### 2.如何使用Ack机制

spout 在发送数据的时候带上msgid

设置acker数至少大于0；Config.setNumAckers(conf, ackerParal);

在bolt中完成处理tuple时，执行OutputCollector.ack(tuple), 当失败处理时，执行OutputCollector.fail(tuple); 

推荐使用IBasicBolt， 因为IBasicBolt 自动封装了OutputCollector.ack(tuple), 处理失败时，抛出

FailedException，则自动执行OutputCollector.fail(tuple)

### 3.如何关闭Ack机制

spout发送数据是不带上msgid

设置acker数等于0

### 4.spout与bolt的其他开发方式

对于spout，有ISpout，IRichSpout，**BaseRichSpout**
对于bolt，有IBolt，IRichBolt，BaseRichBolt，**IBasicBolt**，**BaseBasicBolt**
IBasicBolt，BaseBasicBolt不用每次execute完成都写ack/fail，因为已经帮你实现好了。

## 三.storm与hdfs的整合

如何关闭selinux?

vim /etc/selinux/config

selinux=false

### 1.导入jar包

```
<dependencies>
  	<!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client -->
	<dependency>
	    <groupId>org.apache.hadoop</groupId>
	    <artifactId>hadoop-client</artifactId>
	    <version>2.7.5</version>
	</dependency> 
  	
  
  <!--  use new kafka spout code -->
		  <dependency>
		    <groupId>org.apache.storm</groupId>
		    <artifactId>storm-core</artifactId>
		    <version>1.1.1</version>
		    <scope>provided</scope> 
		</dependency>
		
		 <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>0.10.0.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-kafka-client</artifactId>
            <version>1.1.1</version>
        </dependency>
		<dependency>
		    <groupId>com.alibaba</groupId>
		    <artifactId>fastjson</artifactId>
		    <version>1.2.41</version>
		</dependency>
		 <dependency>
		    <groupId>redis.clients</groupId>
		    <artifactId>jedis</artifactId>
		    <version>2.9.0</version>
		</dependency>
  <!-- https://mvnrepository.com/artifact/org.apache.storm/storm-hdfs -->
 <dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-hdfs</artifactId>
    <version>1.1.1</version>
    <exclusions>
    	<exclusion>
    		<groupId>org.apache.hadoop</groupId>
    		<artifactId>hadoop-client</artifactId>
    	</exclusion>
    	<exclusion>
    		<groupId>org.apache.hadoop</groupId>
    		<artifactId>hadoop-auth</artifactId>
    	</exclusion>	
    	<exclusion>
    		<groupId>org.apache.hadoop</groupId>
    		<artifactId>hadoop-common</artifactId>
    	</exclusion>
    	<exclusion>
    		<groupId>org.apache.hadoop</groupId>
    		<artifactId>hadoop-hdfs</artifactId>
    	</exclusion>
    </exclusions>
	</dependency> 

	<dependency>
			<groupId>org.apache.storm</groupId>
			<artifactId>storm-jdbc</artifactId>
			<version>1.1.1</version>
		</dependency>
		<!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java -->
		<dependency>
		    <groupId>mysql</groupId>
		    <artifactId>mysql-connector-java</artifactId>
		    <version>5.1.38</version>
		</dependency>
  </dependencies>
  
  
  
  <build>
  	<plugins>  
            <plugin>  
 				<groupId>org.apache.maven.plugins</groupId>  
                <artifactId>maven-compiler-plugin</artifactId>  
                <version>3.1</version>  
                <configuration>  
                    <source>1.8</source>  
                    <target>1.8</target> 
                    <encoding>utf-8</encoding> 
                </configuration>  
            </plugin>  
        
        
        <!-- storm与hdfs的整合，请使用这种打包方式 -->
        
         <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>1.4</version>
                <configuration>
                    <createDependencyReducedPom>true</createDependencyReducedPom>
                </configuration>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <transformers>
                                <transformer
                                        implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                                <transformer
                                        implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>cn.itcast.hadoop.stormhadoop.MainTopology</mainClass>
                                </transformer>
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin> 
        
       <!--  
         注意  storm与hdfs的整合已经不能使用这种打包方式，使用这种打包方式会出错   Error preparing HdfsBolt: No FileSystem for scheme: hdfs at org.apache.storm.hdfs.bolt.AbstractHdfs
     storm与mysql的整合请使用这种打包方式
      -->
    <!--    <plugin>
                      <artifactId> maven-assembly-plugin </artifactId>
                      <configuration>
                           <descriptorRefs>
                                <descriptorRef>jar-with-dependencies</descriptorRef>
                           </descriptorRefs>
                           <archive>
                                <manifest>
                                     <mainClass>cn.itcast.storm.demo2.JdbcTopo</mainClass>
                                </manifest>
                           </archive>
                      </configuration>
                      <executions>
                           <execution>
                                <id>make-assembly</id>
                                <phase>package</phase>
                                <goals>
                                     <goal>single</goal>
                                </goals>
                           </execution>
                      </executions>
                 </plugin>  -->
        </plugins>  
  </build>

```

要使用exclusions来排除storm-hdfs中与hadoop中重复的发生冲突的jar包

### 2.发送订单spout

```
public class RandomSpout extends BaseRichSpout{
	
	/**
	 * 
	 */
	private static final long serialVersionUID = -5626424588633283763L;
	
	private SpoutOutputCollector collector;
	
	public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {
        this.collector = collector;    
	}

	public void nextTuple() {
           String orderDetail  = new PaymentInfo().random();
           collector.emit(new Values(orderDetail));
       
	}

	public void declareOutputFields(OutputFieldsDeclarer declarer) {
		 declarer.declare(new Fields("orderLine"));
	}

}
```

### 3.定义统计金额bolt

```
public class MoneyCountBolt extends BaseRichBolt{
	
	private OutputCollector collector;
	
	private String record;
	
	private Map<String,Long> map ;
	private JSONObject obj;
	
	@Override
	public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
		this.collector = collector;
		map =  new HashMap<String,Long>();
		obj = new JSONObject();
		
	}

	@Override
	public void execute(Tuple input) {
		record = input.getStringByField("orderLine");
		collector.emit(new Values(record));
		collector.ack(input);
		PaymentInfo paymentInfo  = obj.parseObject(record, PaymentInfo.class);
		long payPrice = paymentInfo.getPayPrice();
		if(map.containsKey("totalPrice")){
			map.put("totalPrice", map.get("totalPrice")+payPrice);
		}else{
			map.put("totalPrice", payPrice);
		}
		System.out.println("销售总金额为"+map.toString());
		  DateFormat df = new SimpleDateFormat("yyyy-MM-dd_HH-mm-ss");
          Date d = new Date(System.currentTimeMillis());
          String minute = df.format(d);
          collector.emit(new Values(record));
          collector.ack(input);
	}

	@Override
	public void declareOutputFields(OutputFieldsDeclarer declarer) {
		declarer.declare(new Fields("orderDetail"));
	}

}
```

### 4.主代码运行逻辑

```
public class MainTopology {
	
   public static void main(String[] args) throws Exception {
        RecordFormat format = new DelimitedRecordFormat().withFieldDelimiter("\t");
        // 当数据达到100条，写入hdfs
        SyncPolicy syncPolicy = new CountSyncPolicy(100);
        // 当文件大小达到1 KB，就写入HDFS
        FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(1.0f, Units.KB);
        FileNameFormat fileNameFormat = new DefaultFileNameFormat().withPath("/yunzhs/");
        HdfsBolt hdfsBolt = new HdfsBolt()
                .withFsUrl("hdfs://192.168.217.228:9000")
                .withFileNameFormat(fileNameFormat)
                .withRecordFormat(format)
                .withRotationPolicy(rotationPolicy)
                .withSyncPolicy(syncPolicy);
        TopologyBuilder builder = new TopologyBuilder();
        builder.setSpout("event-spout", new RandomSpout(), 3);
        builder.setBolt("countPriceBolt", new MoneyCountBolt(),2).fieldsGrouping("event-spout",new Fields("orderLine"));
        builder.setBolt("hdfs-bolt", hdfsBolt, 2).fieldsGrouping("countPriceBolt", new Fields("orderDetail"));
        Config conf = new Config();
        String name = HdfsBolt.class.getSimpleName();
        if (args != null && args.length > 0) {
          //  conf.put(Config.NIMBUS_HOST, args[0]);
             conf.setNumWorkers(3);
             conf.setDebug(false);
             StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
        } else {
             conf.setMaxTaskParallelism(3);
             LocalCluster cluster = new LocalCluster();
             cluster.submitTopology(name, conf, builder.createTopology());
             Thread.sleep(60000);
             cluster.shutdown();
        }
   }

}
```

![52163933804](/img/posts/1521639338043.png)

![52163935094](/img/posts/1521639350947.png)