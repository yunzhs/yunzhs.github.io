---
layout:     post
title:      kafka学习使用
date:       2017-09-23
author:     yunzhs
header-img: img/Fate of Princess.jpg
catalog: true
tags:
    - bigdata相关
typora-root-url: ..
typora-copy-images-to: ..\img\posts
---

## 1.kafka介绍

 Apache Kafka是一个开源**消息**系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。

 **Kafka**是一个分布式消息队列：生产者、消费者的功能。它提供了类似于**JMS**的特性，但是在设计实现上完全不同，此外它并不是**JMS**规范的实现。

Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。

无论是kafka集群，还是producer和consumer都依赖于**zookeeper**集群保存一些meta信息，来保证系统可用性

## 2.消息队列的作用

消息系统的核心作用就是三点:解耦,异步和并行.

以用户注册的案列来说明消息队列的作用

### 2.1用户注册的一般流程

![52137279157](/img/posts/1521372791574.png)

**问题**：

随着后端流程越来越多，每步流程都需要额外的耗费很多时间，从而会导致用户更长的等待延迟

### 2.2用户注册的并发执行

![52137288423](/img/posts/1521372884232.png)

**问题**：

​	系统并行的发起了4个请求，4个请求中，如果某一个环节执行1分钟，其他环节再快，用户也需要等待1分

钟。如果其中一个环节异常之后，整个服务挂掉了

### 2.3用户注册的最终一致

![52137293544](/img/posts/1521372935442.png)

1.**保证主流程的正常执行**、执行成功之后，发送MQ消息出去。

2.需要这个destination的其他系统通过消费数据再执行，最终一致。

![52137304976](/img/posts/1521373049767.png)

## 3.kafka与activeMQ的区别

传统的消息队列基于pub与sub的过程，kafka是一种全新的概念，基于push与pull的过程

Kafka设计上支持集群并发操作，吞吐率远高于ActiveMQ（以及其他JMS实现）

kafka可靠性上还有些问题，不支持事务，不是JMS规范的东西

## 4.kafka的架构

![52137340219](/img/posts/1521373402198.png)

Producer ：生产者，主要是用于生产我们的数据，与我们activeMQ的生产者类似，数据源主要有flume等

topic：某一类消息的集合，可以总结为这个topic就是我们指定的某一类消息

broker：说白了就是kafka当中的一台台的服务器，叫做broker，也就是我们kafka集群当中一个个的节点

partition：分区的概念，与我们hadoop当中的分区类似，一台服务器存不下我们的所有的消息，所以把消息拆成一个个的分区，分开存放到多台机器上

segment：一个topic有多个分区，一个分区有多个segment（段落），segment就是一个个的日志文件，一个segment由.log于.index两个文件

​	.log：我们的日志文件和数据文件

​	.index：存放我们的偏移量，以及记录我们的消息的信息，索引我们的.log文件，保证我们查找消息快速

zookeeper：主要记录我们kafka集群的一些元数据信息

consumer：消费者，类似于我们activeMQ当中的消费者一样的作用，也是通过指定某一类topic进行消费

kafka当中的消费组的概念

一个partition只能被一个组里面的一个线程消费

一个partition可以被多个组进行消费

offset偏移量：记录我们消费的起始值，也就是说每次我们消费完数据之后，都要更新offset，告诉我们的kafka集群，我消费到哪里来了，下一次来继续上一次的消费，避免造成重复消费的问题

这个offset的记录问题：在kafka当中有两种消费模式，一种是快速消费，这种方式会把我们的offset记录到zookeeper当中去，一种是慢速消费，这种方式会把offset记录到kafka文件当中去，默认使用快速消费

## 5.kafka的安装

**1.在多个机器上搭建zookeeper集群,安装过程略**

**2.下载kafka安装压缩包**

<http://archive.apache.org/dist/kafka/>

**3.上传压缩包并解压**

这里使用kafka_2.11-1.0.0.tgz 这个版本

**4.进入config中更改配置文件**

所有机器均按一下配置进行添加修改,根据自己的集群配置进行修改

```
broker.id=0
log.dirs=/export/servers/kafka_2.11-1.0.0/logs
zookeeper.connect=node01:2181,node02:2181,node03:2181
delete.topic.enable=true
host.name=node-01
```

启动命令

```
nohup  bin/kafka-server-start.sh config/server.properties > /dev/null 2>&1 &
```

**nohup** 后台启动命令一般与**&**  搭配使用,作用就是不用单独的再打开一个会话

**/dev/null**  linux当中的一个类似于黑洞的东西，任何文件或者数据丢进去都会丢失找不到

**2>&1** kafka启动时候的标准日志以及错误日志

## 6.kafka当中一些命令行的使用

kafka当中创建topic

```
./kafka-topics.sh  --create --partitions 3 --replication-factor 2 --topic test --zookeeper node01:2181,node02:2181,node03:2181
```

replication-factor副本因子,给几存几

kafka当中生产消息

```
./kafka-console-producer.sh  --broker-list node-01:9092,node-02:9092,node-03:9092 --topic test
```

kafka当中的消息的消费

```
./kafka-console-consumer.sh  --bootstrap-server node-01:9092,node-02:9092,node-03:9092 --from-beginning --topic test
```

## 7.kafka当中的API

### 第一步：导入jar包的坐标

```
<dependencies>
    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka-clients</artifactId>
        <version>0.11.0.1</version>
    </dependency>
</dependencies>
```

### 第二步：生产者实例代码

```
public static void main(String[] args) {
        Properties props = new Properties();  //alt   +  enter  错误提示
        props.put("bootstrap.servers", "node01:9092,node02:9092,node03:9092");
        props.put("acks", "all");  //ack的取值，可以有三个 1 ，  0 ， -1
        props.put("retries", 0);
        props.put("batch.size", 16384);
        props.put("linger.ms", 1);
        props.put("buffer.memory", 33554432);
        //如果需要自定义分区，一定要记得手动指定我们的分区类
        props.put("partitioner.class", "cn.itcast.partition.MyOwnPartitioner");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        Producer<String, String> producer = new KafkaProducer<String,String>(props);
        for (int i = 0; i < 100; i++){
            //   new ProducerRecord<String,String>();  //ProducerRecord  有很多构造器，最简单的一个，就是两个参数的，第一个参数表示topic的名字，第二个参数表示我们需要放松的数据
           //通过指定分区来进行我们的数据划分
          //  ProducerRecord<String, String> record = new ProducerRecord<String, String>("test", 2, null, i + "helloworld");
            //注意，如果通过key的hash取值来进行分区，那么key的取值，一定要变动，如果不变动，所有的数据都去了一个分区，就会造成数据倾斜
         //   ProducerRecord<String, String> record = new ProducerRecord<String, String>("test", "1", i + "helloworld");
            //没有给定key，也没有指定分区号，通过轮询的方式来进行分区
            ProducerRecord<String, String> record = new ProducerRecord<String, String>("test", i + "helloworld");

            producer.send(record);
        }
        producer.close();
    }


```

### 第三步：消费者实例代码

```
public class KafkaConsumerStudy {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "node01:9092,node02:9092,node03:9092");
        props.put("group.id", "yun04");//指定我们的消费组
        props.put("enable.auto.commit", "true");//允许自动提交我们的offset 偏移量   记录我们的消费的数据在哪一条，在kafka当中，先提交offset偏移量，再进行消费
        props.put("auto.commit.interval.ms", "1000");//指定每秒钟提交我们的offset
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        KafkaConsumer<String, String> consumer = new KafkaConsumer<String,String>(props);
        consumer.subscribe(Arrays.asList("test"));//  定于我们哪一类主题
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(100);//通过poll主动拉取数据//meiyou pull
            for (ConsumerRecord<String, String> record : records)
               // System.out.println("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
               System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
        }
    }
}

```

## 8.kafka当中的分区策略

第一种分区策略：发送消息的时候没有给定key值，通过手动指定分区号的方式来进行分区

```
//通过指定分区来进行我们的数据划分
//  ProducerRecord<String, String> record = new ProducerRecord<String, String>("test", 2, null, i + "helloworld");
```

第二种分区策略：通过我们的key的hash取值来进行分区

```
  //注意，如果通过key的hash取值来进行分区，那么key的取值，一定要变动，如果不变动，所有的数据都去了一个分区，就会造成数据倾斜
//   ProducerRecord<String, String> record = new ProducerRecord<String, String>("test", "1", i + "helloworld");
```

第三种分区策略：没有指定分区，也没有给key，就直接轮询的方式来进行分区

```
//没有给定key，也没有指定分区号，通过轮询的方式来进行分区
ProducerRecord<String, String> record = new ProducerRecord<String, String>("test", i + "helloworld");
```

第四种：自定义分区策略

新建一个继承自Partitioner的类

```
public class KafkaCustomPartitioner implements Partitioner {
	@Override
	public void configure(Map<String, ?> configs) {
	}

	@Override
	public int partition(String topic, Object arg1, byte[] keyBytes, Object arg3, byte[] arg4, Cluster cluster) {
		List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
	    int partitionNum = partitions.size();
		Random random = new Random();
		int partition = random.nextInt(partitionNum);
	    return partition;
	}

	@Override
	public void close() {
		
	}

}
```

在生产者中加入配置

```
props.put("partitioner.class", "cn.itcast.partition.MyOwnPartitioner");
```

## 9.kafka配置文件的说明

Server.properties配置文件说明:

```
#broker的全局唯一编号，不能重复
broker.id=0

#用来监听链接的端口，producer或consumer将在此端口建立连接
port=9092

#处理网络请求的线程数量
num.network.threads=3

#用来处理磁盘IO的现成数量
num.io.threads=8

#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400

#接受套接字的缓冲区大小
socket.receive.buffer.bytes=102400

#请求套接字的缓冲区大小
socket.request.max.bytes=104857600

#kafka运行日志存放的路径
log.dirs=/export/data/kafka/

#topic在当前broker上的分片个数
num.partitions=2

#用来恢复和清理data下数据的线程数量
num.recovery.threads.per.data.dir=1

#segment文件保留的最长时间，超时将被删除
log.retention.hours=1

#滚动生成新的segment文件的最大时间
log.roll.hours=1

#日志文件中每个segment的大小，默认为1G
log.segment.bytes=1073741824

#周期性检查文件大小的时间
log.retention.check.interval.ms=300000

#日志清理是否打开
log.cleaner.enable=true

#broker需要使用zookeeper保存meta数据
zookeeper.connect=zk01:2181,zk02:2181,zk03:2181

#zookeeper链接超时时间
zookeeper.connection.timeout.ms=6000

#partion buffer中，消息的条数达到阈值，将触发flush到磁盘
log.flush.interval.messages=10000

#消息buffer的时间，达到阈值，将触发flush到磁盘
log.flush.interval.ms=3000

#删除topic需要server.properties中设置delete.topic.enable=true否则只是标记删除
delete.topic.enable=true

#此处的host.name为本机IP(重要),如果不改,则客户端会抛出:Producer connection to localhost:9092 unsuccessful 错误!
host.name=kafka01

advertised.host.name=192.168.140.128
```

producer生产者配置文件说明:

```
#指定kafka节点列表，用于获取metadata，不必全部指定
metadata.broker.list=node01:9092,node02:9092,node03:9092
# 指定分区处理类。默认kafka.producer.DefaultPartitioner，表通过key哈希到对应分区
#partitioner.class=kafka.producer.DefaultPartitioner
# 是否压缩，默认0表示不压缩，1表示用gzip压缩，2表示用snappy压缩。压缩后消息中会有头来指明消息压缩类型，故在消费者端消息解压是透明的无需指定。
compression.codec=none
# 指定序列化处理类
serializer.class=kafka.serializer.DefaultEncoder
# 如果要压缩消息，这里指定哪些topic要压缩消息，默认empty，表示不压缩。
#compressed.topics=

# 设置发送数据是否需要服务端的反馈,有三个值0,1,-1
# 0: producer不会等待broker发送ack 
# 1: 当leader接收到消息之后发送ack 
# -1: 当所有的follower都同步消息成功后发送ack. 
request.required.acks=0 

# 在向producer发送ack之前,broker允许等待的最大时间 ，如果超时,broker将会向producer发送一个error ACK.意味着上一次消息因为某种原因未能成功(比如follower未能同步成功) 
request.timeout.ms=10000

# 同步还是异步发送消息，默认“sync”表同步，"async"表异步。异步可以提高发送吞吐量,
也意味着消息将会在本地buffer中,并适时批量发送，但是也可能导致丢失未发送过去的消息
producer.type=sync

# 在async模式下,当message被缓存的时间超过此值后,将会批量发送给broker,默认为5000ms
# 此值和batch.num.messages协同工作.
queue.buffering.max.ms = 5000

# 在async模式下,producer端允许buffer的最大消息量
# 无论如何,producer都无法尽快的将消息发送给broker,从而导致消息在producer端大量沉积
# 此时,如果消息的条数达到阀值,将会导致producer端阻塞或者消息被抛弃，默认为10000
queue.buffering.max.messages=20000

# 如果是异步，指定每次批量发送数据量，默认为200
batch.num.messages=500

# 当消息在producer端沉积的条数达到"queue.buffering.max.meesages"后 
# 阻塞一定时间后,队列仍然没有enqueue(producer仍然没有发送出任何消息) 
# 此时producer可以继续阻塞或者将消息抛弃,此timeout值用于控制"阻塞"的时间 
# -1: 无阻塞超时限制,消息不会被抛弃 
# 0:立即清空队列,消息被抛弃 
queue.enqueue.timeout.ms=-1


# 当producer接收到error ACK,或者没有接收到ACK时,允许消息重发的次数 
# 因为broker并没有完整的机制来避免消息重复,所以当网络异常时(比如ACK丢失) 
# 有可能导致broker接收到重复的消息,默认值为3.
message.send.max.retries=3

# producer刷新topic metada的时间间隔,producer需要知道partition leader的位置,以及当前topic的情况 
# 因此producer需要一个机制来获取最新的metadata,当producer遇到特定错误时,将会立即刷新 
# (比如topic失效,partition丢失,leader失效等),此外也可以通过此参数来配置额外的刷新机制，默认值600000 
topic.metadata.refresh.interval.ms=60000

consumer消费者配置详细说明
# zookeeper连接服务器地址
zookeeper.connect=zk01:2181,zk02:2181,zk03:2181
# zookeeper的session过期时间，默认5000ms，用于检测消费者是否挂掉
zookeeper.session.timeout.ms=5000
#当消费者挂掉，其他消费者要等该指定时间才能检查到并且触发重新负载均衡
zookeeper.connection.timeout.ms=10000
# 指定多久消费者更新offset到zookeeper中。注意offset更新时基于time而不是每次获得的消息。一旦在更新zookeeper发生异常并重启，将可能拿到已拿到过的消息
zookeeper.sync.time.ms=2000
#指定消费 
group.id=itcast
# 当consumer消费一定量的消息之后,将会自动向zookeeper提交offset信息 
# 注意offset信息并不是每消费一次消息就向zk提交一次,而是现在本地保存(内存),并定期提交,默认为true
auto.commit.enable=true
# 自动更新时间。默认60 * 1000
auto.commit.interval.ms=1000
# 当前consumer的标识,可以设定,也可以有系统生成,主要用来跟踪消息消费情况,便于观察
conusmer.id=xxx 
# 消费者客户端编号，用于区分不同客户端，默认客户端程序自动产生
client.id=xxxx
# 最大取多少块缓存到消费者(默认10)
queued.max.message.chunks=50
# 当有新的consumer加入到group时,将会reblance,此后将会有partitions的消费端迁移到新  的consumer上,如果一个consumer获得了某个partition的消费权限,那么它将会向zk注册 "Partition Owner registry"节点信息,但是有可能此时旧的consumer尚没有释放此节点, 此值用于控制,注册节点的重试次数. 
rebalance.max.retries=5

# 获取消息的最大尺寸,broker不会像consumer输出大于此值的消息chunk 每次feth将得到多条消息,此值为总大小,提升此值,将会消耗更多的consumer端内存
fetch.min.bytes=6553600

# 当消息的尺寸不足时,server阻塞的时间,如果超时,消息将立即发送给consumer
fetch.wait.max.ms=5000
socket.receive.buffer.bytes=655360
# 如果zookeeper没有offset值或offset值超出范围。那么就给个初始的offset。有smallest、largest、anything可选，分别表示给当前最小的offset、当前最大的offset、抛异常。默认largest
auto.offset.reset=smallest
# 指定序列化处理类
derializer.class=kafka.serializer.DefaultDecoder
```

## 10.kafka当中的ack机制

在kafka当中，我们可以通过配置ack保证我们的生产者的数据不丢失

request.required.acks=0

这个配置有三个取值：

0：消息发送没有确认机制

1：如果配置成1表示主节点确认保存好了消息

-1： 如果配置成-1，表示主节点和从节点都保存好了消息才会继续发送

## 11.flume与kafka的整合

实现flume监控某个目录下面的所有文件，然后将文件收集发送到kafka消息系统中

配置flume.conf

```
#为我们的source channel  sink起名
a1.sources = r1
a1.channels = c1
a1.sinks = k1
#指定我们的source收集到的数据发送到哪个管道
a1.sources.r1.channels = c1
#指定我们的source数据收集策略
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /export/servers/flumedata
a1.sources.r1.deletePolicy = never
a1.sources.r1.fileSuffix = .COMPLETED
a1.sources.r1.ignorePattern = ^(.)*\\.tmp$
a1.sources.r1.inputCharset = GBK
#指定我们的channel为memory,即表示所有的数据都装进memory当中
a1.channels.c1.type = memory
#指定我们的sink为kafka  sink，并指定我们的sink从哪个channel当中读取数据
a1.sinks.k1.channel = c1
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.topic = test
a1.sinks.k1.kafka.bootstrap.servers = node-01:9092,node-02:9092,node-03:9092
a1.sinks.k1.kafka.flumeBatchSize = 20
a1.sinks.k1.kafka.producer.acks = 1

```

启动flume

```
bin/flume-ng agent --conf conf --conf-file conf/flume.conf --name a1 -Dflume.root.logger=INFO,console
```

## 12.kafka如何保证数据的不丢失

producer如何保证数据的不丢失：
分为两种模式，同步模式与异步模式
同步模式：配置ack，发送一条确认一条，然后再发送下一条数据
异步模式：配置ack，发送一批，确认一批，如果哪一条数据出问题，再单独重新发发送这一条数据即可

broker如何保证数据不丢失：数据备份，副本机制

consumer如何保证数据不丢失：offset，配置offset提交的时间长短，offset数据量的大小，记录了我们每次消费到了哪一条数据来了，下一次过来接着上一次的消费记录进行消费即可

## 13.kafkaManager监控工具的安装与使用

第一步：上传压缩包并解压

unzip kafka-manager-1.3.3.7.zip -d../servers/

第二步：更改application.conf配置里面的zk连接地址

kafka-manager.zkhosts="node-01:2181,node-02:2181,node-03:2181"

第三步：启动kafkaManager

记得先把RUNNING_PID删掉再启动

```
nohup ./kafka-manager  -Dconfig.file=/export/servers/kafka-manager-1.3.3.7/conf/application.conf -Dhttp.port=8070  >/dev/null 2>&1 &

nohup ./kafka-manager  -Dconfig.file=/export/servers/kafka-manager/conf/application.conf -Dhttp.port=8070 &
```

第四步：webUI页面访问

```
http://node-01:8070/
```

![52138337710](/img/posts/1521383377107.png)

第五步:添加一个集群

![52138341765](/img/posts/1521383417658.png)

只需填一个集群,其他按默认即可