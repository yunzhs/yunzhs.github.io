---
layout:     post
title:      用java爬虫搞起点中文网(外加一个爬网易新闻)
subtitle:   
date:       2017-12-26
author:     yunzhs
header-img: img/Teresa & Frida.jpg
catalog: true
tags:
    - 爬虫
typora-root-url: ..
typora-copy-images-to: ..\img\posts
---

## 网络爬虫的原理

- 网络爬虫本质
  - 模拟浏览器发送http请求。
    - 发起请求，返回的二进制输入流。
    - 将输入二进制信息，转换成string。
  - 模拟解析html文档。
    - string转化成一个document对象。
    - 通过操作document对象获取数据。
      - 获取标题 document.getElementById("id_title");
- 网络爬虫的原理（运行机制）
  - 获取单个页面的逻辑
    - 获取数据本身。（你关注的数据）
    - 获取其它页面的URL
## 起点中文的爬虫开发过程
### 开发逻辑

- 获取排行榜页面的所有读书信息，得到是书名，id，url。
- 想获取每本书的数据。
  - 访问上一步获得url，进入书的介绍页。
  - 发现有个免费试读的按钮，直接获取按钮对应的url。
  - 根据获取，进入第一章。解析第一章数据。
    - 解析小说章节的内容
    - 解析下一章的url。

### 技术点

- 多次使用httpGet请求得到相应的数据。
  - 排行榜的html
  - 小说介绍html
  - 每个章节html
- 多次使用jsoup解析页面。
  - 得到所有文章信息（得到是书名，id，url）

  - 得到免费试读的按钮url

  - 得到小说内容及下一章url

##项目实战

### 1.创建maven项目,并导入相关依赖

```
<dependencies>
    <dependency>
        <groupId>org.apache.httpcomponents</groupId>
        <artifactId>httpclient</artifactId>
        <version>4.5.4</version>
    </dependency>
    <dependency>
        <groupId>org.apache.httpcomponents</groupId>
        <artifactId>fluent-hc</artifactId>
        <version>4.5.4</version>
    </dependency>
    <dependency>
        <!-- jsoup HTML parser library @ https://jsoup.org/ -->
        <groupId>org.jsoup</groupId>
        <artifactId>jsoup</artifactId>
        <version>1.10.3</version>
    </dependency>
</dependencies> 
```

### 2.初步解析网页内容

```
// 1. 指定url
HttpGet httpGet = new HttpGet("https://www.qidian.com/rank");
// 2. 发起网络请求
CloseableHttpClient hc = HttpClients.createDefault();
CloseableHttpResponse res = hc.execute(httpGet);
// 3. 得到结果集
String html = EntityUtils.toString(res.getEntity(), Charset.forName("utf-8"));
// 4. 解析内容
Document doc = Jsoup.parse(html);

```

```
String html = Request.Get("http://www.qidian.com/rank").execute().returnContent().asString(Charset.forName("utf-8"));
```

**其中全三步可以用上面的代码直接完成**

若不想用get而想用post则

```
 ArrayList<BasicNameValuePair> parameters = new ArrayList<BasicNameValuePair>();
      parameters.add(new BasicNameValuePair("username", "zhangsan"));
      parameters.add(new BasicNameValuePair("passwd", "123"));
      httpPost.setEntity(new UrlEncodedFormEntity(parameters));
```



**String html = EntityUtils.toString(res.getEntity(), Charset.forName("utf-8"));返回给string的内容:**

返回的是你服务端以流的形式写出的响应正文中的内容，比如在服务端调用的方法最后为：

```
responseWriter.write("this is response body");
```
那在httpclient客户端  这里就会打印出this is response body 这句话。

## 3.顺手推车的不断深入爬虫

```
// 解析排行榜数据
Elements divs = doc.select("[class^=rank-list-row]");
Element div = divs.get(0);
// System.out.println(div);
// 再次发现所有书籍的信息上使用了 class=name 这样样式。
Elements books = div.select(".name");
Elements divs = doc.select("[class^=rank-list-row]");
Element div = divs.get(0);
// System.out.println(div);
// 再次发现所有书籍的信息上使用了 class=name 这样样式。
Elements books = div.select(".name");
for (Element book : books) {
    String bookId = book.attr("data-bid");
    String name = book.text();
    System.out.println("bookId:" + bookId + " name:" + name + "url: https://book.qidian.com/info/" + bookId);
    // 打开一本新书，获取免费阅读的url。
    String url = "https://book.qidian.com/info/" + bookId;
    HttpGet bookHttpGet = new HttpGet(url);
    CloseableHttpClient bookHttpClient = HttpClients.createDefault();
    CloseableHttpResponse bookRes = bookHttpClient.execute(bookHttpGet);
    String bookHtml = EntityUtils.toString(bookRes.getEntity(), "utf-8");
    Document bookDoc = Jsoup.parse(bookHtml);
    // 获取免费阅读按钮的url地址
    Elements startUrls = bookDoc.select("#readBtn");
    String nextUrl = "http:" + startUrls.get(0).attr("href");
    System.out.println("-------------我是免费阅读的url:" + nextUrl);
    // ----循环获取一本书的所有内容
    while (nextUrl != null) {
        CloseableHttpClient client = HttpClients.createDefault();
        CloseableHttpResponse response = client.execute(new HttpGet(nextUrl));
        String content = EntityUtils.toString(response.getEntity(), "utf-8");
        // 发现返回依然是html文档，需要解析文档。
        Document contentDoc = Jsoup.parse(content);
        // 得到两个值 文章内容、下一个url（下一章）
        String title = contentDoc.select(".j_chapterName").get(0).text();
        System.out.println(title);
        // 获取文档的数据，直接找到父标签。class="read-content j_readContent"
        String result = contentDoc.select("[class^=read-content]").get(0).text();
        System.out.println(result);
        System.out.println();
        System.out.println();
        System.out.println();
        System.out.println();
        nextUrl = "http:" + contentDoc.select("#j_chapterNext").get(0).attr("href");
    }

}
```

## 网易新闻爬虫(emmmm,网易别改网站应该就没事 :sweat: )

```
package cn.yunzhs;

import org.apache.http.client.methods.CloseableHttpResponse;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.CloseableHttpClient;
import org.apache.http.impl.client.HttpClients;
import org.apache.http.util.EntityUtils;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;

import java.io.IOException;
import java.nio.charset.Charset;

public class Spider163 {
    public static void main(String[] args) throws IOException {
        HttpGet httpGet=new HttpGet("http://www.163.com");
        CloseableHttpClient ch= HttpClients.createDefault();
        CloseableHttpResponse res=ch.execute(httpGet);
        String html= EntityUtils.toString(res.getEntity(), Charset.forName("utf-8"));
        //System.out.println(html);
        Document doc= Jsoup.parse(html);
        Elements divs=doc.select("[class^=mod_news_tab]");
        Element div=divs.get(0);
        Elements news=div.select("[class^=tab_panel current]");
        for(Element new1:news){
            Elements new2=new1.select("a");
            for(Element new3:new2){
                String url = new3.attr("href");
                //System.out.println("这篇文章的网址为:"+url);
                HttpGet httpGet1=new HttpGet(url);
                CloseableHttpClient ch1= HttpClients.createDefault();
                CloseableHttpResponse res1=ch1.execute(httpGet1);
                String html1= EntityUtils.toString(res1.getEntity(), Charset.forName("utf-8"));
                Document doc1= Jsoup.parse(html1);
                Elements contents=doc1.select("[class^=post_text]");
                System.out.println(contents.size());
                if(contents.size()!=0) {
                    Element content = contents.get(0);
                    String cont = content.text();
                    //String cont = doc1.select("[class^=post_text]").get(0).text();
                    System.out.println(cont);
                }
                else
                    continue;

            }
        }
        ch.close();


    }
}
```

## 附录

Selector选择器概述

:heavy_exclamation_mark:   ns|tag: 通过标签在命名空间查找元素，比如：可以用 fb|name 语法来查找 元素

:heavy_exclamation_mark:   id: 通过ID查找元素，比如：#logo

:heavy_exclamation_mark:   class: 通过class名称查找元素，比如：.masthead

:heavy_exclamation_mark:   [attribute]: 利用属性查找元素，比如：[href]

:heavy_exclamation_mark:   [^attr]: 利用属性名前缀来查找元素，比如：可以用[^data-] 来查找带有HTML5 Dataset属性的元素

:heavy_exclamation_mark:   [attr=value]: 利用属性值来查找元素，比如：[width=500]

:heavy_exclamation_mark:   [attr^=value], [attr$=value], [attr=value]: 利用匹配属性值开头、结尾或包含属性值来查找元素，比如：[href=/path/]

:heavy_exclamation_mark:   [attr~=regex]: 利用属性值匹配正则表达式来查找元素，比如： img[src~=(?i).(png|jpe?g)]

:heavy_exclamation_mark:   *: 这个符号将匹配所有元素

:heavy_exclamation_mark:   Seector选择器组合使用

:heavy_exclamation_mark:   el#id: 元素+ID，比如： div#logo

:heavy_exclamation_mark:   el.class: 元素+class，比如： div.masthead

:heavy_exclamation_mark:   el[attr]: 元素+class，比如： a[href]

:heavy_exclamation_mark:   任意组合，比如：a[href].highlight

:heavy_exclamation_mark:   ancestor child: 查找某个元素下子元素，比如：可以用.body p 查找在"body"元素下的所有 p元素

:heavy_exclamation_mark:   parent > child: 查找某个父元素下的直接子元素，比如：可以用div.content > p 查找 p 元素，也可以用body > 查找body标签下所有直接子元素

:heavy_exclamation_mark:   siblingA + siblingB: 查找在A元素之前第一个同级元素B，比如：div.head + div

:heavy_exclamation_mark:   siblingA ~ siblingX: 查找A元素之前的同级X元素，比如：h1 ~ p

:heavy_exclamation_mark:   el, el, el:多个选择器组合，查找匹配任一选择器的唯一元素，例如：div.masthead, div.logo